[{"id":0,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/questions/01/","title":"01","section":"questions","content":" 如何在kubernetes中如何抓取服务的metrics # prometheus 中自带的几个metrics # prometheus 如何将数据发送给远端存储 # 介绍一下prometheus WAL 机制 # prometheus 使用什么压缩算法压缩数据 # "},{"id":1,"href":"/docs/go/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/golang-benchmark/","title":"benchmark","section":"性能测试","content":" Go 基准测试（Benchmark）原理与实践 # 📈 go test -bench 能让你在纳秒级别洞察代码性能。本指南汇总了从运行原理到实战技巧的完整知识点。\n1 — 核心概念速览 # 关键对象 作用 go test -bench 启动基准测试运行器（benchmark harness）。 func BenchmarkXxx(b *testing.B) 声明一个基准测试函数，名称需以 Benchmark 前缀开头。 b.N 运行器自动决定的循环次数；被测代码需放入 for i := 0; i \u0026lt; b.N; i++ { … }。 计时/统计 默认统计 wall‑clock 时间（ns/op）；使用 -benchmem 可额外输出 B/op 与 allocs/op。 常用辅助 API b.ResetTimer()、b.StopTimer()/b.StartTimer()、b.ReportAllocs()、b.SetBytes(n)、b.RunParallel() 2 — Benchmark 运行原理 # 编译阶段\ngo test 会为测试与基准生成临时 main 包并编译。 探测合适的迭代次数 (b.N)\n初始化 N = 1；如果一次耗时 \u0026lt; 0.5 s，就按指数级放大（×20、×100 …）。 目标：单次基准 ≥ 1 s（可通过 -benchtime 调整），兼顾统计显著性与总时长。 精准计时\n默认计时整个基准函数；调用 StopTimer 后暂停，StartTimer 恢复。 任何 GC、系统调用也被计入；避免在计时区内执行 I/O、网络等操作。 结果输出\n示例：\nBenchmarkNaive-8 1234567 100.0 ns/op -benchmem 追加每次迭代的分配字节数与次数。若调用 SetBytes(n)，还会显示 MB/s。\n并行模式\nb.RunParallel(func(pb *testing.PB){ … }) 会根据 GOMAXPROCS 启动多 goroutine，并通过 pb.Next() 控制循环。\n3 — 如何书写 \u0026amp; 运行基准 # // concat_test.go package concat import ( \u0026#34;strings\u0026#34; \u0026#34;testing\u0026#34; ) func naive(s1, s2 string) string { return s1 + s2 } func buffered(s1, s2 string) string { var buf strings.Builder buf.WriteString(s1) buf.WriteString(s2) return buf.String() } func BenchmarkNaive(b *testing.B) { s1, s2 := \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34; for i := 0; i \u0026lt; b.N; i++ { _ = naive(s1, s2) } } func BenchmarkBuffered(b *testing.B) { s1, s2 := \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34; b.ReportAllocs() for i := 0; i \u0026lt; b.N; i++ { _ = buffered(s1, s2) } } 3.1 运行命令 # # 仅跑基准（跳过单元测试） go test -run=^$ -bench=. ./... # 查看内存分配 go test -run=^$ -bench=. -benchmem ./... # 自定义持续时间（至少 3 s） go test -run=^$ -bench=. -benchtime=3s ./... # 生成 CPU / 内存剖面 go test -run=^$ -bench=. -cpuprofile cpu.out -memprofile mem.out go tool pprof cpu.out 4 — 常用技巧与坑 # 做法 理由 将准备工作放在循环外或使用 StopTimer/StartTimer 避免把非核心逻辑计入基准时间 使用结果变量（如 var sink any） 防止编译器死代码消除 (DCE) 固定输入规模 提高可重复性；多规模可用 b.Run 注意共享状态 \u0026amp; GC 共享 map、随机数种子等会放大噪声 在实体机上运行 VM、笔记本省电模式可能抖动 结合 go test -count=N 多次运行取平均/中位提高置信度 5 — 进阶能力 # 5.1 子基准 # func BenchmarkSizes(b *testing.B) { for _, size := range []int{1 \u0026lt;\u0026lt; 10, 1 \u0026lt;\u0026lt; 20, 1 \u0026lt;\u0026lt; 24} { b.Run(fmt.Sprintf(\u0026#34;%dB\u0026#34;, size), func(b *testing.B) { data := make([]byte, size) for i := 0; i \u0026lt; b.N; i++ { _ = hash(data) } }) } } 5.2 并行吞吐测试 # func BenchmarkPool(b *testing.B) { pool := sync.Pool{New: func() any { return make([]byte, 1024) }} b.RunParallel(func(pb *testing.PB) { for pb.Next() { buf := pool.Get().([]byte) // ... 使用 buf ... pool.Put(buf) } }) } 5.3 结果可视化 # 使用官方工具 benchstat / benchcmp 对比两次基准差异：\ngo test -bench=. \u0026gt; old.txt # 修改代码… go test -bench=. \u0026gt; new.txt benchstat old.txt new.txt 6 — 小结 # 基准测试集成于 testing 包，无需额外依赖。 把 真正欲测的语句 放进 for i \u0026lt; b.N；其余逻辑用 StopTimer 隔离。 -benchmem、pprof 与 benchstat 可帮助你定位瓶颈与分配热点。 基准 ≠ 压测：它关注微观性能；系统级负载需另行压测。 "},{"id":2,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/client-go/informer/deltafifo/","title":"DeltaFIFO","section":"informer","content":" DeltaFIFO # "},{"id":3,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/develop/","title":"develop","section":"prometheus","content":" prometheus 源码解读 # "},{"id":4,"href":"/docs/go/","title":"Go","section":"文档首页","content":"本章节介绍 Go 的基本使用。\n"},{"id":5,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/install/","title":"install","section":"operate","content":"欢迎使用kubernetes\n"},{"id":6,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/operator/install/","title":"install","section":"operator","content":" prometheus 源码解读 # "},{"id":7,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/install/kubernetes/","title":"kubernetes","section":"install","content":" 部署前准备 # 修改机器名 hostnamectl set-hostname master 关闭swap swapoff -a sed -i \u0026#39;/swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab free -m # 检查是否关闭swap ssh允许Root 登陆 vim /etc/ssh/sshd_config 增加 Port 22 PermitRootLogin yes passwd # 设置root 密码 关闭防火墙 sudo ufw disable 修改apt 源地址 cp /etc/apt/sources.list /etc/apt/sources.list.bak # 使用vim 替换/etc/apt/sources.list中的资源地址 vim /etc/apt/sources.list deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse # 更新 vim /etc/resolv.conf # 添加一下两行 nameserver 8.8.8.8 nameserver 8.8.4.4 apt-get update 添加 Kubernetes apt 存储库 sudo tee /etc/apt/sources.list.d/kubernetes.list \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main EOF apt-get update The following signatures couldn\u0026#39;t be verified because the public key is not available: NO_PUBKEY B53DC80D13EDEF05 NO_PUBKEY FEEA9169307EA071 # 如果报错以上错误，执行下面一条命令 recv-keys 是报错现实的NO_PUBKEY sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FEEA9169307EA071 安装kubectl kubeadm kubelet sudo apt-get install -y kubelet=1.22.2-00 kubeadm=1.22.2-00 kubectl=1.22.2-00 安装docker apt-get install -y docker.io systemctl enable docker master # 安装master 节点 kubeadm init \\ --apiserver-advertise-address=192.168.31.239 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.32.3 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 node # 安装node 节点 kubeadm join 192.168.31.208:6443 --token u1soc8.1xyeqbzptpvvjz8f --discovery-token-ca-cert-hash sha256:3406b992b2c05f27b398a81375082a72aa2a823fd82616e4f8cb6f2a24370bb3 # kubeadm init 初始化完成会打印到console 上 # 可以通过 kubeadm token create --print-join-command 查看 "},{"id":8,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/","title":"Kubernetes","section":"云原生","content":"kubernetes 架构图\nGroup、Version、Resource 核心数据结构 # 整个Kubernetes 体系架构中，资源是Kubernetes最重要的概念，可以说Kubernets的生态系统都围绕着资源运作。\nKubernetes系统虽然有相当复杂和众多的功能，但是它本质上是一个资源控制系统——注册、管理、调度资源并维护资源的状态\nKubernetes 将资源分组和版本化，形成Group（资源组）、Version（资源版本）、Resource（资源）\nGroup：资源组，在Kubernetes API Server 中也可称其为APIGroup Version：资源版本，在Kubernets API Server 中也可称其为APIVersions Resource：资源，在Kubernetes API Server 中也可称其为APIResource Kind：资源种类，描述Resource 的种类，与Resource为同一级别 Kubernetes 系统支持多个Group。每个Group支持多个Version，每个Version支持多个Resource，同时部分资源会拥有自己的子资源\n资源组、资源版本、资源、子资源的完整表达形式为\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;resource\u0026gt;/\u0026lt;status\u0026gt;,以常用的Deployment资源为例，其完整表现形式为apps/v1/deployments/status\n资源对象由 资源组 + 资源版本 + 资源种类组成，例如Deployment 资源是实例化后拥有资源组、资源版本及资源种类，其表现形式为/,Kind=,例如apps/v1，Kind= Deployment\n每一个资源都拥有一定数量的资源操作方法（即Verbs），资源操作方法用于Etcd集群存储中对资源对象的增、删、改、查操作。目前Kubernetes系统支持8种资源操作方法，分别是create、delete、deletecollection、get、list、patch、update、watch操作方法\n"},{"id":9,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/operator/install/kubernetes-services/","title":"kubernetes service","section":"install","content":"scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: # 提取 annotation 中的 metrics_path - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ # 提取 annotation 中的 scrape interval - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_interval] action: replace target_label: scrape_interval # 提取 annotation 中的 scrape timeout - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_timeout] action: replace target_label: scrape_timeout # 通过 annotation 设置是否启用 scrape - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: \u0026#34;true\u0026#34; # 提取 annotation 中的 metrics 端口 - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+) replacement: $1 apiVersion: v1 kind: Pod metadata: name: example-app annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/scrape_interval: \u0026#34;15s\u0026#34; prometheus.io/scrape_timeout: \u0026#34;10s\u0026#34; spec: containers: - name: example image: example-app ports: - containerPort: 8080 "},{"id":10,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%9B%9E%E6%BA%AF/1.-n-%E7%9A%87%E5%90%8E/","title":"N 皇后","section":"回溯","content":" N 皇后 # leetcode 51\n按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或同一斜线上的棋子。\nn 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。\n给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。\n每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 \u0026lsquo;Q\u0026rsquo; 和 \u0026lsquo;.\u0026rsquo; 分别代表了皇后和空位。\n示例 1： 输入：n = 4\n输出：[[\u0026quot;.Q..\u0026quot;,\u0026quot;\u0026hellip;Q\u0026quot;,\u0026ldquo;Q\u0026hellip;\u0026rdquo;,\u0026quot;..Q.\u0026quot;],[\u0026quot;..Q.\u0026quot;,\u0026ldquo;Q\u0026hellip;\u0026rdquo;,\u0026quot;\u0026hellip;Q\u0026quot;,\u0026quot;.Q..\u0026quot;]]\n解释：如上图所示，4 皇后问题存在两个不同的解法。\n示例 2：\n输入：n = 1\n输出：[[\u0026ldquo;Q\u0026rdquo;]]\nfunc solveNQueens(n int) [][]string { var result [][]string board := make([][]rune, n) for i := range board{ board[i] = make([]rune, n) for j := range board[i]{ board[i][j] = \u0026#39;.\u0026#39; } } var dfs func(row int, cols, diag1, diag2 map[int]bool) dfs = func(row int, cols, diag1, diag2 map[int]bool){ if row == n{ var solution []string for _, r := range board{ solution = append(solution, string(r)) } result = append(result, solution) return } for col := 0; col \u0026lt; n; col ++{ if cols[col] || diag1[row-col] || diag2[row+col]{ continue } board[row][col] = \u0026#39;Q\u0026#39; cols[col], diag1[row-col], diag2[col + row] = true, true, true dfs(row + 1, cols, diag1, diag2) board[row][col] = \u0026#39;.\u0026#39; delete(cols, col) delete(diag1, row -col) delete(diag2, row + col) } } dfs(0, make(map[int]bool), make(map[int]bool), make(map[int]bool)) return result } "},{"id":11,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/operator/","title":"operator","section":"prometheus","content":" prometheus 源码解读 # "},{"id":12,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-cd/plan/","title":"plan","section":"argo cd","content":" 📘 Argo CD 学习计划 - 第 2 周 # 🎯 学习目标 # 理解 Argo CD 的核心架构与 GitOps 原理。 能够独立安装 Argo CD 并连接 Git 仓库部署 Kubernetes 应用。 掌握同步策略、状态管理、RBAC 和多环境部署技巧。 📅 每日任务安排 # ✅ 周一：Argo CD 概述与架构 # 学习内容：\n什么是 GitOps？与 CI/CD 的区别 Argo CD 架构图：API Server、Repository Server、Application Controller、UI 支持的 Git 类型（GitHub、GitLab）、Helm/Kustomize 支持 参考资料：\n官方文档：https://argo-cd.readthedocs.io/ 架构图参考： ✅ 周二：安装 Argo CD # 安装命令（在新集群或同集群 namespace 区分）：\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 访问 UI（端口转发）：\nkubectl port-forward svc/argocd-server -n argocd 8080:443 获取初始 admin 密码：\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 访问地址： https://localhost:8080\n✅ 周三：部署第一个应用 # 示例 Git 仓库：https://github.com/argoproj/argocd-example-apps\n使用 CLI 创建 app：\nargocd login localhost:8080 argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default 同步应用：\nargocd app sync guestbook UI 查看状态：同步状态、健康状态\n✅ 周四：同步策略与健康检查 # 手动同步 vs 自动同步\nsyncPolicy 示例：\nsyncPolicy: automated: prune: true selfHeal: true Hook 与生命周期操作（pre-sync, post-sync 等）\n查看状态：\nargocd app get guestbook ✅ 周五：RBAC 与多环境管理 # 配置用户权限：使用 argocd-rbac-cm 配置 mapRoles\n示例：只读用户、admin 用户\n多环境支持：使用 Git 分支、Kustomize 或 Helm value 分文件夹分环境\nUI 多项目配置：Project 管理、限制目标 namespace\n📝 总结建议 # 每天完成操作截图记录、UI 状态记录。 思考 Argo CD 在实际工作中如何对接你的项目。 可尝试将两个不同 Git 仓库应用配置到同一 Argo CD 管理中。 "},{"id":13,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-workflow/plan/","title":"plan","section":"argo workflow","content":" 📘 Argo Workflows 学习计划 - 第 1 周 # 🎯 学习目标 # 理解 Argo Workflows 的基本概念、架构和组件。 能够在本地或 Kubernetes 集群中安装并运行 Workflows。 掌握编写基础 Workflow YAML 的能力，了解参数化、模板复用及定时调度任务（CronWorkflow）。 📅 每日任务安排 # ✅ 周一：Workflows 概述与架构 # 学习内容：\nArgo Workflows 是什么？解决什么问题？ 工作流的基本术语（模板、步骤、参数等） 架构组件：Workflow Controller、UI Server、CRD 资料：\n官方文档：https://argoproj.github.io/argo-workflows/ 架构图参考： ✅ 周二：安装 Argo Workflows # 操作步骤：\nkubectl create namespace argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/install.yaml kubectl -n argo port-forward deployment/argo-server 2746:2746 访问 UI：http://localhost:2746\n验证安装：\nkubectl get pods -n argo ✅ 周三：创建第一个 Workflow # 示例 YAML：\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026#34;hello world\u0026#34;] 提交命令：\nkubectl create -f hello-world.yaml -n argo 查看执行：Argo UI 或 kubectl get wf -n argo\n✅ 周四：参数与模板复用 # 使用参数：\nargs: parameters: - name: message value: \u0026#34;Hello Argo\u0026#34; 模板复用：定义多个模板并通过 steps 引用\n示例项目：https://github.com/argoproj/argo-workflows/tree/master/examples\n✅ 周五：定时任务（CronWorkflow） # 示例 YAML：\napiVersion: argoproj.io/v1alpha1 kind: CronWorkflow metadata: name: hello-cron spec: schedule: \u0026#34;*/5 * * * *\u0026#34; workflowSpec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026#34;Hello from cron!\u0026#34;] 提交命令：\nkubectl create -f cron-workflow.yaml -n argo 验证命令：kubectl get cronworkflow -n argo\n📝 总结建议 # 每晚完成操作记录一份运行截图或 UI 截图。 笔记记录遇到的问题和解决方法。 可尝试将 Workflow 与其他容器任务组合成多步骤流程。 "},{"id":14,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/plan/","title":"plan","section":"Argo","content":" Argo 全家桶学习计划 📘（4 周） # 👤 基础情况 # 起始日期：2025-05-** 学习目标：掌握 Argo Workflows、CD、Rollouts、Events，具备 GitOps 实践能力 每天学习时间：工作日晚上 7:30 ~ 9:15，周末可补充 背景：具备 Kubernetes 基础 ✅ 学习进度一览 # 周次 主题 状态 第 1 周 Argo Workflows ☐ 未开始 / ✅ 完成 第 2 周 Argo CD ☐ 未开始 / ✅ 完成 第 3 周 Argo Rollouts \u0026amp; Events ☐ 未开始 / ✅ 完成 第 4 周 综合实战与优化 ☐ 未开始 / ✅ 完成 🗓 每周学习安排 # 📖 第 1 周：Argo Workflows # 日期 内容 状态 笔记链接 周一 Workflows 概述与架构 ☐ 周二 安装 Argo Workflows ☐ 周三 编写第一个 Workflow ☐ 周四 参数化与模板复用 ☐ 周五 CronWorkflow 与调度 ☐ 📖 第 2 周：Argo CD # 日期 内容 状态 笔记链接 周一 Argo CD 架构与 GitOps 理解 ☐ 周二 安装与配置 Argo CD ☐ 周三 GitOps 应用部署实践 ☐ 周四 同步策略与健康检查 ☐ 周五 RBAC 与多环境支持 ☐ 📖 第 3 周：Argo Rollouts \u0026amp; Events # 日期 内容 状态 笔记链接 周一 Rollouts 概述与策略对比 ☐ 周二 蓝绿发布实践 ☐ 周三 金丝雀发布 + 指标分析 ☐ 周四 Argo Events 结构理解 ☐ 周五 实战：Webhook/File/Kafka 触发 Workflow ☐ 📖 第 4 周：综合实践 # 日期 内容 状态 笔记链接 周一 整体联动流程构建 ☐ 周二 模拟 CI/CD 流程 ☐ 周三 安全性 \u0026amp; 权限管理 ☐ 周四 性能优化 \u0026amp; 指标监控 ☐ 周五 项目演练总结 \u0026amp; 文档输出 ☐ 📝 每日笔记模板（可复制多份使用） # ### 📅 日期：2025-05-xx #### 📚 今日学习内容 - [ ] 主题： - [ ] 目标： - [ ] 操作记录： - [ ] 报错 \u0026amp; 解决方案： #### 🧠 今日总结 - 收获： - 疑问： #### 🧰 明日计划 - [ ] ... "},{"id":15,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/","title":"prometheus","section":"metrics","content":"prometheus 架构图\n"},{"id":16,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/operator/install/prometheus/","title":"prometheus","section":"install","content":" prometheus 的安装 # 使用 kube-prometheus 安装\n组件 # prometheus-adpter prometheus-operator alertmanager blackbox-exporter grafana prometheus kube-state-metrics node-exporter kubernetetsControllerPlan "},{"id":17,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/develop/source-code/","title":"prometheus source code","section":"develop","content":" 模块列表 # discoveryManagerScrape 服务发现 scrapeManager queryEngine ruleManager discoveryManagerNotify notifierManager tracingManager "},{"id":18,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/reources/","title":"resources","section":"develop","content":" Group、Version、Resource 核心数据结构 # 整个Kubernetes 体系架构中，资源是Kubernetes最重要的概念，可以说Kubernets的生态系统都围绕着资源运作。\nKubernetes系统虽然有相当复杂和众多的功能，但是它本质上是一个资源控制系统——注册、管理、调度资源并维护资源的状态\nKubernetes 将资源分组和版本化，形成Group（资源组）、Version（资源版本）、Resource（资源）\nGroup：资源组，在Kubernetes API Server 中也可称其为APIGroup Version：资源版本，在Kubernets API Server 中也可称其为APIVersions Resource：资源，在Kubernetes API Server 中也可称其为APIResource Kind：资源种类，描述Resource 的种类，与Resource为同一级别 Kubernetes 系统支持多个Group。每个Group支持多个Version，每个Version支持多个Resource，同时部分资源会拥有自己的子资源\n资源组、资源版本、资源、子资源的完整表达形式为\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;resource\u0026gt;/\u0026lt;status\u0026gt;,以常用的Deployment资源为例，其完整表现形式为apps/v1/deployments/status\n资源对象由 资源组 + 资源版本 + 资源种类组成，例如Deployment 资源是实例化后拥有资源组、资源版本及资源种类，其表现形式为/,Kind=,例如apps/v1，Kind= Deployment\n每一个资源都拥有一定数量的资源操作方法（即Verbs），资源操作方法用于Etcd集群存储中对资源对象的增、删、改、查操作。目前Kubernetes系统支持8种资源操作方法，分别是create、delete、deletecollection、get、list、patch、update、watch操作方法\nKubernetes的资源也分两种，分别是Kubernetes Resource（kubernetes 内置资源） 和 Custom Resource（自定义资源）。开发者通过CRD（即Custom Resource Definitions）可实现自定义资源，它允许用户将自己定义的资源添加到Kubenetes 系统中，并像使用kubernetes 内置资源一样使用它们\n"},{"id":19,"href":"/docs/linux/%E7%BD%91%E7%BB%9C/tcp%E5%8D%8F%E8%AE%AE/","title":"tcp 协议","section":"网络","content":" 详解三次握手，四次挥手 # 三次握手（建立连接） # TCP采用 三次握手（Three-Way Handshake） 过程来建立可靠连接：\n客户端发送 SYN（同步序列号）： 客户端向服务器发送一个 SYN 报文，表示请求建立连接。 该报文的 SYN=1，并包含一个初始序列号 seq=x。 服务器回应 SYN-ACK： 服务器收到 SYN 后，发送 SYN=1, ACK=1，并附带自己的初始序列号 seq=y，确认客户端的 seq=x+1。 客户端回应 ACK： 客户端收到服务器的 SYN-ACK 后，发送 ACK=1，确认 seq=y+1，同时携带 seq=x+1。 四次挥手（断开连接） # TCP 采用 四次挥手（Four-Way Handshake） 断开连接：\n客户端发送 FIN： 客户端想要断开连接，发送 FIN=1，表示无数据可发。 服务器发送 ACK： 服务器收到 FIN 后，回复 ACK=1，但可能仍有未发送的数据。 服务器发送 FIN： 服务器确认数据发送完毕，发送 FIN=1 断开连接。 客户端发送 ACK，完成断开： 客户端确认 FIN，回复 ACK=1，连接正式关闭。 "},{"id":20,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/questions/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%97%A0%E7%8A%B6%E6%80%81%E6%9C%8D%E5%8A%A1kubernetes%E7%9A%84%E5%86%85%E9%83%A8%E6%B5%81%E7%A8%8B/","title":"创建一个无状态服务","section":"questions","content":" 创建一个deployment资源 # 当创建一个deployment时，kubernetes 内部是如何处理的？\n创建 Deployment 资源 用户通过 kubectl apply 或 API 直接创建一个 Deployment 资源。 kube-apiserver 接收到请求后，将 Deployment 对象写入 etcd 数据库。 controller-manager 处理 Deployment kube-controller-manager 监听 Deployment 资源（通过 watch 机制）。 发现新建的 Deployment，DeploymentController 计算出需要创建的 ReplicaSet（如果是更新，还会进行滚动升级）。 controller-manager 通过 kube-apiserver 创建 ReplicaSet 资源。 controller-manager 处理 ReplicaSet kube-controller-manager 监听 ReplicaSet 资源（通过 watch 机制）。 发现新建的 ReplicaSet，ReplicaSetController 计算出需要创建的 Pod。 controller-manager 通过 kube-apiserver 创建 Pod 资源。 kube-scheduler 进行调度 kube-scheduler 监听 Pod 资源的变更（通过 watch 机制）。 发现 Pod 处于 Pending 状态，调度器计算出合适的 Node（根据 CPU、内存、调度策略等）。 kube-scheduler 通过 kube-apiserver 更新 Pod 的 spec.nodeName，分配到合适的节点。 kubelet 负责拉起容器 kubelet 监听 kube-apiserver，发现被分配到自己节点的 Pod。 kubelet 通过 Container Runtime Interface (CRI) 执行： 创建网络（调用 CNI 插件，分配 IP）。 挂载存储（调用 CSI 插件，挂载 Volume）。 启动容器（调用 CRI，使用 containerd 或 Docker 等创建容器）。 kubelet 通过 kube-apiserver 更新 Pod 状态（Running）。 创建一个service资源 # kube-proxy 监听 kube-apiserver 上的 service 资源的创建： 当 kube-proxy 监视到新的服务资源（Service）时，它会根据 Service 中的标签（labels）来筛选出符合条件的 Pod。这些 Pod 是与该 Service 相关联的目标（后端）。通过这些信息，kube-proxy 创建一个 Endpoints 资源对象，指向这些 Pod。 kube-proxy 监听 endpoint 资源的变化： Endpoints 资源对象被创建或更新，kube-proxy 就会动态地根据新的 Endpoint 信息来修改其网络规则（如 iptables 或 ipvs）。 kube-proxy 根据 Endpoint 中的 Pod 地址创建或更新相应的 iptables 或 ipvs 规则，这样流量就可以通过正确的后端 Pod 进行路由。 "},{"id":21,"href":"/docs/%E7%AE%97%E6%B3%95/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/1.-%E8%85%90%E7%83%82%E7%9A%84%E6%A9%98%E5%AD%90/","title":"腐烂的橘子","section":"拓扑排序","content":" 腐烂的橘子 # leetcode 994\n在给定的 m x n 网格 grid 中，每个单元格可以有以下三个值之一：\n值 0 代表空单元格； 值 1 代表新鲜橘子； 值 2 代表腐烂的橘子。 每分钟，腐烂的橘子 周围 4 个方向上相邻 的新鲜橘子都会腐烂。\n返回 直到单元格中没有新鲜橘子为止所必须经过的最小分钟数。如果不可能，返回 -1 。\n示例 1：\n输入：grid = [[2,1,1],[1,1,0],[0,1,1]]\n输出：4\n示例 2：\n输入：grid = [[2,1,1],[0,1,1],[1,0,1]]\n输出：-1\n解释：左下角的橘子（第 2 行， 第 0 列）永远不会腐烂，因为腐烂只会发生在 4 个方向上。\n示例 3：\n输入：grid = [[0,2]]\n输出：0\n解释：因为 0 分钟时已经没有新鲜橘子了，所以答案就是 0 。\ntype Point struct { x, y, time int } func orangesRotting(grid [][]int) int { rows, cols := len(grid), len(grid[0]) freshCount := 0 queue := []*Point{} directions := [][]int{{1,0}, {-1, 0}, {0, 1}, {0, -1}} totalTime := 0 for i:=0; i \u0026lt; (rows); i++{ for j := 0; j \u0026lt; (cols); j++{ if grid[i][j] == 2{ queue = append(queue, \u0026amp;Point{i, j, 0}) }else if grid[i][j] == 1{ freshCount ++ } } } for len(queue) != 0{ p := queue[0] queue = queue[1:] for _, direction := range directions { newX, newY := p.x + direction[0], p.y + direction[1] if newX \u0026gt;= 0 \u0026amp;\u0026amp; newX \u0026lt; rows \u0026amp;\u0026amp; newY \u0026gt;= 0 \u0026amp;\u0026amp; newY \u0026lt; cols \u0026amp;\u0026amp; grid[newX][newY] == 1{ grid[newX][newY] = 2 queue = append(queue, \u0026amp;Point{newX, newY, p.time + 1}) freshCount -- totalTime = p.time + 1 } } } if freshCount != 0{ return -1 } return totalTime } "},{"id":22,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%8F%8C%E6%8C%87%E9%92%88/1.%E6%8E%A5%E9%9B%A8%E6%B0%B4/","title":"接雨水","section":"双指针","content":" 接雨水 # leetcode 45\n给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n示例 1：\n输入：height = [0,1,0,2,1,0,1,3,2,1,2,1] 输出：6 解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例 2：\n输入：height = [4,2,0,3,2,5] 输出：9\nfunc trap(height []int) int { left, right := 0, len(height) -1 leftMax, rightMax := 0, 0 totalWater := 0 for left \u0026lt; right{ if height[left] \u0026lt; height[right]{ if height[left] \u0026lt; leftMax{ totalWater += leftMax - height[left] }else{ leftMax = height[left] } left ++ } else{ if height[right] \u0026gt;= rightMax{ rightMax = height[right] }else{ totalWater += rightMax - height[right] } right-- } } fmt.Println(left, right) return totalWater } "},{"id":23,"href":"/docs/%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%8F%89%E6%A0%91/1.-%E5%89%8D%E5%BA%8F%E9%81%8D%E5%8E%86/","title":"前序遍历","section":"二叉树","content":" 前序遍历 # 根 → 左子树 → 右子树\nfunc preOrderTraversal(root *TreeNode) []int { if root == nil { return []int{} } stack, res := []*TreeNode{root}, []int{} for len(stack) \u0026gt; 0 { node := stack[len(stack)-1] stack = stack[:len(stack)-1] res = append(res, node.Val) if node.Right != nil { stack = append(stack, node.Right) } if node.Left != nil { stack = append(stack, node.Left) } } return res } "},{"id":24,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%A0%86/1.-%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/","title":"数据流的中位数","section":"堆","content":" 数据流的中位数 # leetcode 295\n中位数是有序整数列表中的中间值。如果列表的大小是偶数，则没有中间值，中位数是两个中间值的平均值。\n例如 arr = [2,3,4] 的中位数是 3 。\n例如 arr = [2,3] 的中位数是 (2 + 3) / 2 = 2.5 。\n实现 MedianFinder 类:\nMedianFinder() 初始化 MedianFinder 对象。\nvoid addNum(int num) 将数据流中的整数 num 添加到数据结构中。\ndouble findMedian() 返回到目前为止所有元素的中位数。与实际答案相差 10-5 以内的答案将被接受。\n示例 1：\n输入 [\u0026ldquo;MedianFinder\u0026rdquo;, \u0026ldquo;addNum\u0026rdquo;, \u0026ldquo;addNum\u0026rdquo;, \u0026ldquo;findMedian\u0026rdquo;, \u0026ldquo;addNum\u0026rdquo;, \u0026ldquo;findMedian\u0026rdquo;]\n[[], [1], [2], [], [3], []] 输出 [null, null, null, 1.5, null, 2.0]\n解释 MedianFinder medianFinder = new MedianFinder();\nmedianFinder.addNum(1); // arr = [1]\nmedianFinder.addNum(2); // arr = [1, 2]\nmedianFinder.findMedian(); // 返回 1.5 ((1 + 2) / 2)\nmedianFinder.addNum(3); // arr[1, 2, 3]\nmedianFinder.findMedian(); // return 2.0\ntype MaxHeap struct{ nums []int } func (h MaxHeap) Len() int { return len(h.nums) } func (h MaxHeap) Less(i, j int) bool { return h.nums[i] \u0026gt; h.nums[j] } // 大根堆 func (h MaxHeap) Swap(i, j int) { h.nums[i], h.nums[j] = h.nums[j], h.nums[i] } func (h *MaxHeap) Push(x interface{}) { h.nums = append(h.nums, x.(int)) } func (h *MaxHeap) Pop() interface{} { old := h.nums n := len(old) x := old[n-1] h.nums = old[:n-1] return x } // 小根堆（存较大的一半数据） type MinHeap struct{ nums []int } func (h MinHeap) Len() int { return len(h.nums) } func (h MinHeap) Less(i, j int) bool { return h.nums[i] \u0026lt; h.nums[j] } // 小根堆 func (h MinHeap) Swap(i, j int) { h.nums[i], h.nums[j] = h.nums[j], h.nums[i] } func (h *MinHeap) Push(x interface{}) { h.nums = append(h.nums, x.(int)) } func (h *MinHeap) Pop() interface{} { old := h.nums n := len(old) x := old[n-1] h.nums = old[:n-1] return x } // 数据流中位数 type MedianFinder struct { maxHeap *MaxHeap // 大根堆 minHeap *MinHeap // 小根堆 } // 初始化 func Constructor() MedianFinder { return MedianFinder{ maxHeap: \u0026amp;MaxHeap{}, minHeap: \u0026amp;MinHeap{}, } } // 添加数字 func (mf *MedianFinder) AddNum(num int) { heap.Push(mf.maxHeap, num) // 先放入大根堆 heap.Push(mf.minHeap, heap.Pop(mf.maxHeap)) // 平衡到小根堆 // 如果小根堆元素多于大根堆，则调整 if mf.minHeap.Len() \u0026gt; mf.maxHeap.Len() { heap.Push(mf.maxHeap, heap.Pop(mf.minHeap)) } } // 获取中位数 func (mf *MedianFinder) FindMedian() float64 { if mf.maxHeap.Len() \u0026gt; mf.minHeap.Len() { return float64(mf.maxHeap.nums[0]) } return float64(mf.maxHeap.nums[0]+mf.minHeap.nums[0]) / 2.0 } "},{"id":25,"href":"/docs/%E7%AE%97%E6%B3%95/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","title":"贪心算法","section":"算法","content":"本章节介绍贪心算法\n基本思想 # 局部最优解：在每一步决策中，选择当前状态下最优的局部解 无后效性：当前的选择不会影响后续的选择，即不依赖于未来的选择 希望导致全局最优解：通过一系列局部最优解，最终得到全局最优解 适用条件 # 贪心选择性质：问题的全局最优解可以通过一系列局部最优选择得到 最优子结构：问题的最优解包含子问题的最优解 步骤 # 问题分解：将问题分为若干个子问题 局部最优选择：对每个子问题，选择一个局部最优解 合并解：将局部最优解合并为全局解 经典应用 # 活动选择问题：选择最多的互不重叠的活动。 霍夫曼编码：构造最优前缀编码。 最小生成树：如Prim算法和Kruskal算法。 最短路径问题：如Dijkstra算法。 背包问题的分数版本：选择单位价值最高的物品。 优缺点 # 优点\n简单易实现 通常具有较高的效率 缺点 不总是能得到全局最优解 需要问题具有贪心选择性质和最优子结构 "},{"id":26,"href":"/docs/%E7%AE%97%E6%B3%95/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/first/","title":"跳跃游戏 2","section":"贪心算法","content":" 跳跃游戏 2 # leetcode 45\n给定一个长度为 n 的 0 索引整数数组 nums。初始位置为 nums[0]。\n每个元素 nums[i] 表示从索引 i 向后跳转的最大长度。换句话说，如果你在 nums[i] 处，你可以跳转到任意 nums[i + j] 处:\n0 \u0026lt;= j \u0026lt;= nums[i] i + j \u0026lt; n 返回到达 nums[n - 1] 的最小跳跃次数。生成的测试用例可以到达 nums[n - 1]。\n示例 1:\n输入: nums = [2,3,1,1,4] 输出: 2 解释: 跳到最后一个位置的最小跳跃数是 2。 从下标为 0 跳到下标为 1 的位置，跳 1 步，然后跳 3 步到达数组的最后一个位置。 示例 2:\n输入: nums = [2,3,0,1,4] 输出: 2\nfunc jump(nums []int) int { n := len(nums) if n \u0026lt;= 1{ return 0 } steps , curEnd, maxReach := 0, 0, 0 for i :=0 ; i\u0026lt; n ; i ++{ maxReach = max(maxReach, i + nums[i]) if i == curEnd{ steps ++ curEnd = maxReach if curEnd \u0026gt;= n-1{ break } } } return steps } "},{"id":27,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/questions/%E7%BD%91%E7%BB%9C/","title":"网络","section":"questions","content":" kubernetes 容器提供一个服务，外部访问慢，如何排查是容器网络原因还是服务本身的问题 # 如果容器内部没有sh 和网络工具，如何查看pod 内部的网络连接 # 使用 kubectl debug kubernetes 提供了kubectl debug让你可以在pod的网络namespace 内运行一个临时容器，该容器可以包含网络工具，如netstat、ss、curl、tcpdump等\nkubectl debug -it \u0026lt;pod-name\u0026gt; --image=busybox --target=\u0026lt;container-name\u0026gt; 使用nsenter 进入pod的网络Namespace 如果你有集群节点的权限，可以找到pod运行的节点，并使用nsenter 进入pod的网络的namespace # 先找到 Pod 运行在哪个节点 kubectl get pod \u0026lt;pod-name\u0026gt; -o wide # SSH 进入该节点，然后找到 Pod 对应的容器 ID docker ps | grep \u0026lt;pod-name\u0026gt; # 如果是 Docker 运行时 crictl ps | grep \u0026lt;pod-name\u0026gt; # 如果是 containerd 或 CRI-O 运行时 # 获取网络 namespace，并使用 nsenter 进入 PID=$(docker inspect --format \u0026#39;{{.State.Pid}}\u0026#39; \u0026lt;container-id\u0026gt;) # Docker PID=$(crictl inspect --output go-template --template \u0026#39;{{.info.pid}}\u0026#39; \u0026lt;container-id\u0026gt;) # containerd nsenter -t $PID -n netstat -an nsenter -t $PID -n ss -tnlp 使用tcpdump在Node 上抓包 tcpdump -i any port \u0026lt;pod-port\u0026gt; kubectl get pod \u0026lt;pod-name\u0026gt; -o jsonpath=\u0026#39;{.status.podIP}\u0026#39; tcpdump -i any host \u0026lt;pod-IP\u0026gt; "},{"id":28,"href":"/docs/","title":"文档首页","section":"bei的个人博客","content":" 欢迎使用 Hugo Book 主题！ # "},{"id":29,"href":"/docs/go/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","title":"性能测试","section":"Go","content":"本章节介绍 golang 的性能测试\n"},{"id":30,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%88%86%E6%B2%BB%E6%B3%95/1.-%E5%AF%BB%E6%89%BE%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9C%80%E5%B0%8F%E5%80%BC/","title":"寻找旋转排序数组中的最小值","section":"分治法","content":" 寻找旋转排序数组中的最小值 # leetcode 153\n已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,2,4,5,6,7] 在变化后可能得到：\n若旋转 4 次，则可以得到 [4,5,6,7,0,1,2]\n若旋转 7 次，则可以得到 [0,1,2,4,5,6,7]\n注意，数组 [a[0], a[1], a[2], \u0026hellip;, a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], \u0026hellip;, a[n-2]] 。\n给你一个元素值 互不相同 的数组 nums ，它原来是一个升序排列的数组，并按上述情形进行了多次旋转。请你找出并返回数组中的 最小元素 。\n你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。\n示例 1：\n输入：nums = [3,4,5,1,2]\n输出：1\n解释：原数组为 [1,2,3,4,5] ，旋转 3 次得到输入数组。\n示例 2：\n输入：nums = [4,5,6,7,0,1,2]\n输出：0\n解释：原数组为 [0,1,2,4,5,6,7] ，旋转 4 次得到输入数组。\n示例 3：\n输入：nums = [11,13,15,17]\n输出：11\n解释：原数组为 [11,13,15,17] ，旋转 4 次得到输入数组。\nfunc findMin(nums []int) int { left, right := 0, len(nums) -1 for left \u0026lt; right{ mid := (left + right) / 2 if nums[mid] \u0026gt; nums[right]{ left = mid + 1 }else{ right = mid } } return nums[left] } "},{"id":31,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/1.-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/","title":"最长公共子序列","section":"动态规划","content":" 最长公共子序列 # leetcode 300\n给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。\n一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。\n例如，\u0026ldquo;ace\u0026rdquo; 是 \u0026ldquo;abcde\u0026rdquo; 的子序列，但 \u0026ldquo;aec\u0026rdquo; 不是 \u0026ldquo;abcde\u0026rdquo; 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。\nfunc longestCommonSubsequence(text1 string, text2 string) int { m, n := len(text1), len(text2) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 1; i \u0026lt;= m; i++ { for j := 1; j \u0026lt;= n; j++ { if text1[i-1] == text2[j-1] { dp[i][j] = dp[i-1][j-1] + 1 } else { dp[i][j] = max(dp[i-1][j], dp[i][j-1]) } } } return dp[m][n] } "},{"id":32,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/questions/","title":"Questions","section":"develop","content":" # "},{"id":33,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-workflow/","title":"argo workflow","section":"Argo","content":"介绍使用argo workflow\n"},{"id":34,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/install/calico/","title":"calico","section":"install","content":"xxx\n"},{"id":35,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/cilium/","title":"Cilium","section":"云原生","content":"本章节介绍Cilium\n"},{"id":36,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/client-go/","title":"client-go","section":"develop","content":"欢迎使用client-go\n"},{"id":37,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/controller-runtime/","title":"controller runtime","section":"develop","content":"欢迎使用controller runtime\n"},{"id":38,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/","title":"develop","section":"Kubernetes","content":"欢迎使用kubernetes\n"},{"id":39,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/client-go/informer/indexr/","title":"Indexer","section":"informer","content":" Indexer # "},{"id":40,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/client-go/informer/","title":"informer","section":"client-go","content":" "},{"id":41,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-cd/introduce/","title":"introduce","section":"argo cd","content":" 📘 Argo CD 学习笔记：第 1 天 - GitOps 与架构理解 # ✅ 什么是 GitOps？ # GitOps 是一种通过 Git 仓库作为**单一事实来源（single source of truth）**来管理和部署基础设施和应用程序的模式。其核心理念是：\n声明式配置：使用 Git 保存 Kubernetes 等资源的 YAML 配置 自动部署：当 Git 中配置变化时，系统自动将变化同步到集群中 回滚与审计：基于 Git 的提交记录，可轻松回滚、审计变更历史 📌 GitOps 与 CI/CD 的区别 # 项目 CI/CD GitOps 触发方式 通常基于事件（如 Push）触发流水线 以 Git 为驱动源，通过对比配置实现部署 核心工具 Jenkins/GitLab CI/GitHub Actions 等 Argo CD、Flux 状态同步 外部工具推送状态 Git 中配置是集群的目标状态 回滚机制 依赖流水线和构建产物 直接 git revert 并同步即可 可观测性 CI/CD 工具日志 Git + 状态对比 + Argo CD 界面 🧩 Argo CD 架构图与组件 # 核心组件解释： # 组件 说明 API Server 提供 gRPC 和 REST 接口，与前端交互，处理 CLI/API 请求 Repository Server 拉取 Git 仓库的内容，生成 manifests Application Controller 负责监控和同步 Kubernetes 资源 Argo CD UI 提供直观的 Web 界面用于管理 Application Dex（可选） 用于实现 SSO 登录的身份认证服务 🔧 支持的 Git 类型与模板工具 # 支持的 Git 仓库类型： # ✅ GitHub / GitHub Enterprise ✅ GitLab / GitLab Self-Hosted ✅ Bitbucket / Bitbucket Server ✅ Gitea、Azure DevOps 等 支持认证方式：HTTPS/SSH、Token、用户名密码等\n支持的配置模板技术： # 类型 描述 纯 K8s YAML 最简单的声明式配置 Kustomize 基于 overlay 的配置管理，适合多环境 Helm 模板化部署，支持传值、嵌套模板 Jsonnet JSON-based 编程式配置 插件方式 可自定义 parser 工具，如 kpt、cdk8s 等 📚 推荐阅读 # GitOps 官方定义： https://opengitops.dev Argo CD 架构文档： https://argo-cd.readthedocs.io/en/stable/operator-manual/architecture/ GitOps vs CI/CD 博客：https://www.weave.works/blog/gitops-operations-by-pull-request "},{"id":42,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-event/introduce/","title":"introduce","section":"argo event","content":" 📡 Argo Events 架构理解 # Argo Events 是 Argo 项目的一个事件驱动引擎，专为 Kubernetes 原生环境设计，支持通过外部事件自动触发工作流、部署、任务执行等动作，是构建 GitOps 自动化的关键工具之一。\n✅ 核心概念 # 1. EventSource # 定义事件来源（例如 Webhook、S3 上传、Kafka 消息、Cron 计划任务等），Argo Events 会监听这些事件源并产生事件。\napiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: webhook-source spec: webhook: example: endpoint: /trigger method: POST port: 12000 2. Sensor # Sensor 监听 EventSource 的事件，当满足触发条件时，激活 Trigger。可以设置依赖关系、条件判断。\napiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: webhook-sensor spec: dependencies: - name: example-dep eventSourceName: webhook-source eventName: example triggers: - template: name: trigger-workflow k8s: group: argoproj.io version: v1alpha1 resource: workflows operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: triggered-wf- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026#34;Hello from event\u0026#34;] 3. Trigger # 指定事件触发后的行为，例如启动 Argo Workflow、Kubernetes Job、发送通知等。支持模板化、参数注入、条件触发等复杂逻辑。\n🔄 支持的触发器类型 # 类型 描述 webhook 接收 HTTP 请求（如 GitLab Push、Jenkins 通知） s3 / minio 文件上传触发工作流（例如日志、模型） kafka / nats 通过消息队列异步触发流程 calendar / cron 定时执行任务，代替 CronWorkflow file 本地文件系统变化触发 resource 监听 Kubernetes 对象变化（如 Pod 状态） amqp, mqtt, sqslike 企业集成常用消息源 slack 消息触发自动响应（如 bot 流程） 完整支持列表参考官方文档：https://argoproj.github.io/argo-events/sources/\n🧩 架构图 # 架构简要说明 # 组件 功能 EventSource Controller 创建并监听事件来源 Sensor Controller 检测事件是否满足触发条件 EventBus（NATS） 内部事件总线（可选）用于异步解耦 Trigger Handler 执行具体的触发操作（如创建 Workflow） 📚 推荐资料 # 官方文档：https://argoproj.github.io/argo-events/ GitHub：https://github.com/argoproj/argo-events 示例合集：https://github.com/argoproj/argo-events/tree/master/examples "},{"id":43,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-rollout/introduce/","title":"introduce","section":"argo rollout","content":" 🚀 Argo Rollouts 概述与发布策略 # ✅ 什么是 Argo Rollouts？ # Argo Rollouts 是一个 Kubernetes 控制器，用于实现高级的部署策略，如：\n蓝绿部署（Blue-Green Deployment） 金丝雀发布（Canary Release） 实验分析（Analysis Run） 支持自动回滚、手动推进、AB 测试等能力 它是对原生 Kubernetes Deployment 的增强版，提供更精细的发布控制与观察能力。\n🔄 Deployment vs Rollout # 特性 Kubernetes Deployment Argo Rollouts 基本滚动更新 ✅ 有 ✅ 有 蓝绿发布 ❌ 无 ✅ 支持 金丝雀发布 ❌ 无 ✅ 支持 手动控制发布阶段 ❌ 无 ✅ 支持 自动分析并决策 ❌ 无 ✅ 支持 版本对比和观察窗口 ❌ 无 ✅ 支持 指标判断（Prometheus 等） ❌ 无 ✅ 支持 Web UI 支持 ❌ 无 ✅ 有（Argo Rollouts UI 插件） 实验/AB 测试 ❌ 无 ✅ 支持 🚦 支持的发布策略 # 1. 金丝雀发布（Canary） # 将新版本流量逐步放量，比如 10% -\u0026gt; 30% -\u0026gt; 100%，每个阶段可通过 metrics 分析判断是否推进。\nstrategy: canary: steps: - setWeight: 10 - pause: {duration: 1m} - setWeight: 50 - pause: {} 2. 蓝绿部署（Blue-Green） # 部署新版本于“预览”环境，手动或自动切换流量。\nstrategy: blueGreen: activeService: my-app-active previewService: my-app-preview autoPromotionEnabled: false 3. 实验分析（Analysis Run） # 可插入 Prometheus/Datadog/New Relic 等指标平台，进行自动化判断是否回滚或推进。\nanalysis: templates: - name: success-rate-check metrics: - name: error-rate provider: prometheus: query: rate(http_requests_total{status=~\u0026#34;5..\u0026#34;}[1m]) 🧩 Argo Rollouts 架构图 # 关键组件说明： # 组件 说明 Rollout Controller 控制器核心，替代原生 Deployment 控制器 Rollout CRD 定义替代 Deployment 的高级对象 AnalysisRun 执行分析模板、连接外部指标源 Experiment 并行部署多个版本，用于 AB 测试 UI 插件 可集成至 Argo CD 观察 rollout 状态 📚 推荐阅读 # 官方文档：https://argo-rollouts.readthedocs.io/ GitHub 项目：https://github.com/argoproj/argo-rollouts 示例 YAML：https://github.com/argoproj/argo-rollouts/tree/master/examples "},{"id":44,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-workflow/introduce/","title":"introduce","section":"argo workflow","content":" 📘 Argo Workflows 基础概念 # ✅ Argo Workflows 是什么？ # Argo Workflows 是一个用于在 Kubernetes 上运行 容器化工作流 的开源工具。它允许你通过自定义的 YAML 文件，定义一系列任务的执行流程，这些任务以容器的形式运行，并可以通过步骤或依赖顺序进行编排。\n它主要用于：\nCI/CD 流程编排 数据处理流水线 自动化测试任务 批处理作业运行 机器学习任务执行（如 Kubeflow Pipelines） Argo Workflows 通过 CRD（自定义资源定义）扩展了 Kubernetes，原生集成在集群中运行，无需引入外部服务。\n🔧 基本术语解释 # 术语 含义 Workflow 工作流的实例，是任务的运行单元 Template 可复用的任务模板，每个 template 定义一个操作 Step 指定任务的顺序执行逻辑 DAG 有向无环图，用于定义任务之间的依赖 Parameter 参数变量，允许模板传入数据 Artifact 文件或数据对象，在任务之间传递或保存结果 Entrypoint 定义 Workflow 的起始模板 WorkflowTemplate 可以全局复用的模板定义（跨 Workflow 共享） 示意结构：\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world- spec: entrypoint: hello templates: - name: hello container: image: alpine:3.7 command: [\u0026#34;echo\u0026#34;] args: [\u0026#34;hello world\u0026#34;] 🧩 Argo Workflows 架构组件 # 组件 作用 Workflow Controller 核心控制器，监听并调度 Workflow 的每一步任务 argo-server (UI Server) 提供 Web UI 管理界面，可查看执行状态、日志 CRD 包括 Workflow、WorkflowTemplate、CronWorkflow 等资源，扩展 Kubernetes Executor Sidecar 容器，负责执行每个任务的命令并报告状态 Artifact Repository 任务之间的数据传递（如 S3、OSS、PVC） 📚 推荐学习资源 # 官网文档： https://argoproj.github.io/argo-workflows GitHub 项目： https://github.com/argoproj/argo-workflows 样例库： https://github.com/argoproj/argo-workflows/tree/master/examples "},{"id":45,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/backstage/introduce/","title":"introduce","section":"Backstage","content":" Backstage 详解介绍 # 🎯 什么是 Backstage？ # Backstage 是由 Spotify 开源的开发者门户平台框架，用于构建集中化的开发者体验平台（Developer Portal），帮助组织统一管理软件组件、CI/CD 流程、监控工具、文档等资源。\nGitHub 地址： https://github.com/backstage/backstage\n🧠 Backstage 原理与架构 # Backstage 基于插件架构，提供统一的 UI 界面来聚合不同的开发工具和资源。其核心原理为：\n软件目录（Software Catalog）：注册管理组织内的所有软件组件（如微服务、库、文档等）。 插件机制：通过插件系统实现功能扩展，如 CI/CD、监控、API 管理等。 统一的 UI 界面：基于 React 构建，通过插件动态渲染内容。 配置驱动：基于 YAML 文件配置软件组件元数据。 +---------------------+ | Developer | +---------------------+ | v +---------------------+ +---------------------+ | Backstage Frontend | \u0026lt;-----\u0026gt; | Backstage Backend | +---------------------+ +---------------------+ | | v v +----------------+ +--------------------------+ | Plugins | | Software Catalog + Auth | +----------------+ +--------------------------+ | v +---------------------------+ | External Tools (CI/CD, | | Monitoring, Docs, etc.) | +---------------------------+ 🧩 核心功能模块 # 软件目录（Catalog）：集中注册和管理所有组件。 TechDocs：支持以 Markdown 为基础的文档自动生成和展示。 CI/CD 集成：支持 Jenkins、GitHub Actions、GitLab 等。 监控可视化：集成 Grafana、Prometheus、Sentry 等工具。 权限控制：支持 RBAC、LDAP、OAuth 等。 🧱 技术栈 # 组件 技术 前端 React + TypeScript 后端 Node.js + Express 插件系统 插件基于 React 和 Node 构建 配置管理 YAML 部署 支持 Docker / Kubernetes 🔌 插件机制 # Backstage 插件分为两种：\n前端插件：提供 UI 功能，独立模块化。 后端插件：与外部服务交互，如数据库、CI 服务等。 可以通过 npx @backstage/create-app 快速初始化插件和项目。\n🚀 部署方式 # 支持以下部署方式：\nDocker Compose Kubernetes（Helm Charts 可用） 静态文件 + Node 后端服务 🌐 使用场景 # 多团队协作统一入口 DevOps 工具集成 内部服务发现与治理 自动化文档平台 微服务治理门户 🌍 社区与生态 # 提供丰富的官方插件 支持第三方插件市场 与 CNCF 等社区合作紧密 📚 参考文档 # 官方文档：https://backstage.io/docs 插件市场：https://backstage.io/plugins "},{"id":46,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/certmanager/introduce/","title":"introduce","section":"CertManager","content":" cert-manager 介绍与原理文档 # 一、什么是 cert-manager？ # cert-manager 是一个 Kubernetes 插件，用于自动化管理和颁发 TLS 证书。它可以帮助用户从多个证书颁发机构（如 Let\u0026rsquo;s Encrypt、HashiCorp Vault、自建 CA）自动请求和续签证书，从而确保集群中的服务通信安全。\n官网地址：https://cert-manager.io/\n二、作用 # 自动化 TLS 证书申请和续期 支持多种 Issuer（颁发者）类型，如 ACME（Let\u0026rsquo;s Encrypt）、CA、Vault 等 统一 Kubernetes 中证书的管理方式 配合 Ingress 控制器使用，实现 HTTPS 自动化 与 Kubernetes 原生对象集成，如 Secret、Ingress、Service 等 三、核心概念 # 名称 说明 Issuer 表示一个证书颁发者，可为命名空间级别或全局（ClusterIssuer） Certificate 表示一个证书请求，cert-manager 会据此去申请证书并生成 Secret Challenge ACME 协议中的校验挑战过程，用于验证域名所有权 Order 一个 ACME 协议中发起的证书申请订单 Secret 存储颁发后的证书和私钥，用于挂载到 Pod 或配置到 Ingress 中 四、工作原理 # 五、架构图（Mermaid） # graph TD A[User / YAML] --\u0026gt;|Create Certificate| B[cert-manager Controller] B --\u0026gt; C[Check Issuer or ClusterIssuer] C --\u0026gt; D{Issuer Type} D --\u0026gt;|ACME| E[ACME Solver] E --\u0026gt; F[Challenge + Order] F --\u0026gt; G[ACME CA (e.g., Let\u0026#39;s Encrypt)] D --\u0026gt;|CA / Vault| H[Internal CA / Vault API] G --\u0026gt; I[Signed Certificate] H --\u0026gt; I I --\u0026gt; J[Secret Created / Updated] J --\u0026gt; K[Used by Ingress / Pod] 六、常见使用场景 # 配合 NGINX Ingress Controller 实现自动 HTTPS 自动颁发和续期 Pod 之间 mTLS 所需的证书 对接内部 CA，提供可信任的内部证书签发服务 与 Vault 集成，实现高度安全的证书存储和访问 七、参考资源 # cert-manager 官方文档 Kubernetes Ingress TLS 配置指南 "},{"id":47,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/cilium/introduce/","title":"introduce","section":"Cilium","content":" 🚀 Cilium 简介、架构与作用 # 📌 一、Cilium 是什么？ # Cilium 是一个基于 eBPF 技术构建的云原生网络和安全平台，它作为 Kubernetes 的 CNI 插件，提供了高性能的容器网络连接、网络安全策略、流量可观测性以及 Service Mesh 能力。\n✨ 核心特点： # 🧠 基于 Linux eBPF 技术，性能优异、无侵入 🔐 支持 L3/L4/L7 网络策略（支持 HTTP/gRPC/Kafka 等） ♻️ 无需 Sidecar 的 Service Mesh 能力 👀 内建 Hubble 实现网络流量可视化与追踪 🌐 支持多集群互联（ClusterMesh） 🏗️ 二、Cilium 架构 # Cilium 架构主要由以下几个组件组成：\n1️⃣ Cilium Agent # 每个 Node 上运行 管理和编译 eBPF 程序，注入到内核 处理服务发现、负载均衡、策略同步等 2️⃣ eBPF 程序（运行在内核） # 挂载在 XDP、TC、Socket 等钩子点上 实现数据包的解析、过滤、重定向、转发 性能高，运行在内核态 3️⃣ Hubble # 基于 eBPF 的可观测性平台 收集网络流量事件、DNS、HTTP 等信息 提供 CLI 与 UI 工具，支持 Grafana 集成 4️⃣ Cilium CLI # 管理工具，支持安装、诊断、策略测试等 5️⃣ Cilium Operator # 控制平面组件，处理 CRD、Kubernetes 资源变更 6️⃣ ClusterMesh（多集群） # 实现多个 Kubernetes 集群之间的 Pod 互联 同步服务、策略、身份信息 🛠️ 三、Cilium 的主要作用 # 📌 场景 描述 🔗 CNI 插件 提供容器网络连接，支持 IPv4/IPv6、Overlay、BGP 等 🛡️ 网络安全策略 支持从 L3（IP）到 L7（HTTP Path）的多层访问控制 🧩 Service Mesh 支持无 Sidecar 模式，支持 mTLS、L7 策略、流量控制 🔍 可观测性 Hubble 提供流量追踪、指标、路径分析等功能 🚪 Egress Gateway 精细控制 Pod 的出口访问地址与 IP 🌐 多集群通信 支持 ClusterMesh，多个集群共享网络与策略 📈 四、适用场景 # 🚀 构建高性能 Kubernetes 网络平台 🧵 对服务间通信进行精细化控制（如 L7 HTTP 策略） 🧼 替代传统 Service Mesh 实现零 Sidecar 化 🛡️ 构建零信任网络架构（mTLS + 策略） 🔄 跨集群网络管理 🧪 网络观测、调试与安全审计 🧾 五、总结 # Cilium 是一个集网络连接、安全策略、可视化与服务网格功能于一体的现代化平台，特别适合对性能、安全、可观测性有较高要求的生产集群环境。凭借 eBPF 技术优势，Cilium 正逐渐成为下一代 Kubernetes 网络标准的有力竞争者。\n"},{"id":48,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/","title":"operate","section":"Kubernetes","content":"欢迎使用kubernetes\n"},{"id":49,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","title":"动态规划","section":"算法","content":"本章节介绍贪心算法\n基本原理 # 最优子结构 一个问题的最优解包含其子问题的最优解，换句话说，问题的最优解可以通过子问题的最优解推到出来 重叠子问题 在求解过程中，子问题会被多次重复计算，动态规划通过存储子问题的解（通常使用表格或数组），避免重复计算 适用条件 # 最优化问题： 背包问题（0-1 背包、完全背包） 最短路径问题（Floyd-Warshall 算法） 最长公共子序列（LCS） 最大子数组和 计数问题： 组合数问题 路径计数问题 序列问题： 斐波那契数列 编辑距离（Levenshtein 距离） 步骤 # 定义状态 明确问题的状态表示，通常一个或多个变量描述问题的子问题 确定状态转移方程 找到状态之间的关系，即如何从子问题的解推导出当前问题的解 初始化 确定初始状态的值，通常问题的最小规模情况 计算顺序 按照一定的顺序计算状态，通常是自底向上（从最小子问题开始）或自顶向下（递归 + 记忆化） 返回结果 根据状态或数组返回最终问题的解 优化 # 空间优化： 如果状态转移方程只依赖于前一个状态，可以使用滚动数组或变量来减少空间复杂度。 例如：斐波那契数列问题可以优化为只使用两个变量。 记忆化搜索： 自顶向下的递归方法，通过缓存子问题的解来避免重复计算。 "},{"id":50,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BAkubeconfig/","title":"如何创建kubeConfig","section":"operate","content":" 如何创建kubeConfig # 创建clusterRole kubectl create clusterrole monitor \u0026ndash;resource=prometheusrule \u0026ndash;verb=\u0026quot;*\u0026quot;\n创建clusterRoleBinding kubectl create clusterrolebinding monitor \u0026ndash;clusterrole=rule \u0026ndash;user monitor\n生成普通用户令牌 cd /etc/kubernetes/pki #你的集群证书目录 user=monitor organization=monitoring clustername=cluster02 # 【集群地址，例如：https://172.21.114.169:6443】 api_addr=https://192.168.31.208:6443 time_days=365 #复制粘贴下面的代码即可 umask 077;openssl genrsa -out $user.key 2048 openssl req -new -key $user.key -out $user.csr -subj \u0026#34;/O=$organization/CN=$user\u0026#34; openssl x509 -req -in $user.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out $user.crt -days $time_days kubectl config set-cluster $clustername --server=$api_addr --certificate-authority=ca.crt --embed-certs=true --kubeconfig=/root/$user.config kubectl config set-credentials $user --client-certificate=$user.crt --client-key=$user.key --embed-certs=true --kubeconfig=/root/$user.config kubectl config set-context $user@$clustername --cluster=$clustername --user=$user --kubeconfig=/root/$user.config kubectl config use-context $user@$clustername --kubeconfig=/root/$user.config "},{"id":51,"href":"/docs/go/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"数据结构","section":"Go","content":"本章节介绍 golang 的数据结构\n"},{"id":52,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/","title":"云原生","section":"文档首页","content":" 生产环境云原生部署架构图 # 架构图（文字版） # +--------------------------------------------------------------------------------------------------------+ | Kubernetes 集群 | | | | +---------------------+ +-------------------+ +---------------------+ | | | Cilium CNI |\u0026lt;--\u0026gt;| NetworkPolicy/EBPF |\u0026lt;--\u0026gt;| Pod 网络通信与安全策略 | | | +---------------------+ +-------------------+ +---------------------+ | | | | +---------------------+ | | | cert-manager |----\u0026gt; 自动签发/续期 TLS 证书（整合 ACME / Vault / Issuer） | | +---------------------+ | | | | +---------------------+ | | | Keycloak |----\u0026gt; OIDC/SAML 登录认证（整合 ArgoCD、Harbor、Grafana 等） | | +---------------------+ | | | | +---------------------+ +---------------------+ | | | Harbor |\u0026lt;--\u0026gt; 镜像仓库（私有仓库，结合 Keycloak OIDC） | | | +---------------------+ +---------------------+ | | | | +----------------------+ +----------------------+ +---------------------------+ | | | Argo Workflows | | Argo CD | | Argo Events / Rollouts | | | | (CI流水线调度器) | | (GitOps 部署) | | （事件驱动 / 灰度发布） | | | +----------------------+ +----------------------+ +---------------------------+ | | ↑ GitOps触发构建 ↑ 从Git拉取应用部署配置 ↑ CRDs / 事件驱动CI流程 | | | | +---------------------+ +---------------------+ +--------------------------+ | | | Prometheus |\u0026lt;---| Node Exporter / Kubelet |\u0026lt;-- Kubernetes 指标收集 | | +---------------------+ +---------------------+ +--------------------------+ | | ↓ Alertmanager ↓ Grafana (集成Keycloak登录) | | 告警规则与通知 可视化监控面板 | | | | +---------------------+ | | | Loki |\u0026lt;--- 日志收集（整合 promtail / fluentbit） | | +---------------------+ | | | +--------------------------------------------------------------------------------------------------------+ ☁️ Cloud-Native Production Environment Architecture # 🧩 Central Platform: # 🧱 Kubernetes Cluster: Orchestrates containers and manages workloads. 🛠️ Key Components: # 🧑‍💻 Backstage: Developer portal providing software catalog, CI/CD integration, documentation, and monitoring dashboards. 📦 Harbor: Private container registry to manage container images securely. 🕸️ Cilium: Provides networking, security policies, observability using eBPF technology. 🔐 Cert-manager: Automates the issuance and renewal of TLS certificates. 🛡️ Keycloak: Identity and access management system providing authentication and authorization. 🚀 Argo Suite: 📥 Argo CD: GitOps continuous deployment tool. 🔁 Argo Workflows: Workflow orchestration and CI pipelines. 🧲 Argo Events/Rollouts: Event-driven workflows and canary deployments. 📈 Prometheus: Monitoring and alerting toolkit. 📄 Loki: Log aggregation system for collecting and managing logs. 🔗 Integration and Interactions: # Backstage interacts with all components, providing a unified view for developers and administrators. Harbor is integrated with Kubernetes and Keycloak for secure image management. Cilium manages cluster networking, providing observability data to Prometheus. Cert-manager integrates with Kubernetes and Keycloak for automated certificate management. Keycloak provides centralized authentication services for Kubernetes, Backstage, Harbor, and Argo Suite. Argo Suite integrates with Kubernetes and Git repositories for streamlined deployments and CI/CD processes. Prometheus gathers metrics from all components, with dashboards presented via Backstage. Loki aggregates logs from Kubernetes and applications, accessible via dashboards. ✨ This architecture ensures scalability, security, observability, and ease of use for cloud-native application deployment and management.\n"},{"id":53,"href":"/docs/%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%8F%89%E6%A0%91/2.-%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86/","title":"中序遍历","section":"二叉树","content":" 中序遍历 # 左子树 → 根 → 右子树\nfunc inorderTraversal(root *TreeNode) []int { if root == nil { return []int{} } stack, res := []*TreeNode{}, []int{} cur := root for cur != nil \u0026amp;\u0026amp; len(stack) \u0026gt; 0 { for cur != nil { stack = append(stack, cur) cur = cur.Left } cur = stack[len(stack)-1] stack = stack[:len(stack)-1] res = append(res, cur.Val) cur = cur.Right } return res } "},{"id":54,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/install/argo/","title":"argo","section":"install","content":"xxx\n"},{"id":55,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-cd/","title":"argo cd","section":"Argo","content":"介绍使用argo cd\n"},{"id":56,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/certmanager/","title":"CertManager","section":"云原生","content":"介绍使用CertManager\n"},{"id":57,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/kubelet/cri/containerd/","title":"containerd","section":"CRI","content":"介绍使用containerd作为kubernetes CNI网络插件的使用\n"},{"id":58,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/kubelet/cri/","title":"CRI","section":"operate","content":"介绍使用containerd作为kubernetes CNI网络插件的使用\n"},{"id":59,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/client-go/example/","title":"example","section":"client-go","content":" example code\n创建CRD 结构 /pkg/apis/example/v1/types.go\ntype MyResource struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec MyResourceSpec `json:\u0026#34;spec\u0026#34;` } // MyResourceSpec defines the desired state of MyResource type MyResourceSpec struct { Foo string `json:\u0026#34;foo\u0026#34;` Bar int `json:\u0026#34;bar\u0026#34;` } 使用code-generator 工具自动生成clientSet、informer、lister bash hack/update-codegen.sh 定义controller 结构 /pkg/controller/contronller.go\ntype Controller struct { kubeclientset kubernetes.Interface myresourceClientset clientset.Interface myresourceLister listers.MyResourceLister myresourceSynced cache.InformerSynced workqueue workqueue.TypedRateLimitingInterface[string] } 初始化controller /pkg/controller/contronller.go\nfunc NewController( kubeclientset kubernetes.Interface, myresourceClientset clientset.Interface, myresourceInformer v1.MyResourceInformer) *Controller { controller := \u0026amp;Controller{ kubeclientset: kubeclientset, myresourceClientset: myresourceClientset, myresourceLister: myresourceInformer.Lister(), myresourceSynced: myresourceInformer.Informer().HasSynced, workqueue: workqueue.NewTypedRateLimitingQueueWithConfig( workqueue.DefaultTypedControllerRateLimiter[string](), workqueue.TypedRateLimitingQueueConfig[string]{ Name: \u0026#34;my-controller\u0026#34;, }, ), } myresourceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueMyResource, UpdateFunc: func(old, new interface{}) { controller.enqueueMyResource(new) }, DeleteFunc: controller.enqueueMyResource, }) return controller } set up /metrics and /healthz接口 /pkg/controller/contronller.go\n// Set up metrics server http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) go func() { klog.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) }() // Set up health check server http.HandleFunc(\u0026#34;/healthz\u0026#34;, health.HealthCheck) go func() { klog.Fatal(http.ListenAndServe(\u0026#34;:8081\u0026#34;, nil)) }() 使用leaderelection 编写高可用controller /cmd/my-controller.go\nlock := \u0026amp;resourcelock.LeaseLock{ LeaseMeta: metav1.ObjectMeta{ Name: leaseLockName, Namespace: leaseLockNamespace, }, Client: kubeClient.CoordinationV1(), LockConfig: resourcelock.ResourceLockConfig{ Identity: os.Getenv(\u0026#34;POD_NAME\u0026#34;), // Use the pod name as the identity }, } leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{ Lock: lock, ReleaseOnCancel: true, LeaseDuration: 15 * time.Second, RenewDeadline: 10 * time.Second, RetryPeriod: 2 * time.Second, Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { klog.Info(\u0026#34;Started leading\u0026#34;) informerFactory.Start(ctx.Done()) // Start the controller logic }, OnStoppedLeading: func() { klog.Info(\u0026#34;Stopped leading\u0026#34;) le \u0026lt;- struct{}{} }, OnNewLeader: func(identity string) { if identity == os.Getenv(\u0026#34;POD_NAME\u0026#34;) { klog.Info(\u0026#34;I am the leader\u0026#34;) } else { klog.Infof(\u0026#34;New leader elected: %s\u0026#34;, identity) } }, }, }) 启动controller /cmd/my-controller.go\nmyController.Run(2, ctx.Done()) 业务逻辑写在 pkg/controller/contronller.go\nfunc (c *Controller) syncHandler(key string) error { namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { runtime.HandleError(fmt.Errorf(\u0026#34;invalid resource key: %s\u0026#34;, key)) return nil } myresource, err := c.myresourceLister.MyResources(namespace).Get(name) if err != nil { if errors.IsNotFound(err) { runtime.HandleError(fmt.Errorf(\u0026#34;myresource \u0026#39;%s\u0026#39; in work queue no longer exists\u0026#34;, key)) return nil } return err } fmt.Printf(\u0026#34;Sync/Add/Update for MyResource %s\\n\u0026#34;, myresource.GetName()) return nil } "},{"id":60,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-cd/install/","title":"install","section":"argo cd","content":" 🚀 Argo CD + ApplicationSet 安装指南（生产推荐） # 本指南包含 Argo CD 与 ApplicationSet Controller 的完整安装流程，适用于生产环境部署。\n✅ 一、准备工作 # Kubernetes 集群（v1.21+ 推荐） Helm 3.x 具备集群管理员权限 域名（如使用 SSO 或 Ingress） 🧰 二、添加 Argo Helm 仓库 # helm repo add argo https://argoproj.github.io/argo-helm helm repo update 📦 三、安装 Argo CD（含 ApplicationSet） # helm install argocd argo/argo-cd --namespace argocd --create-namespace --set server.service.type=ClusterIP --set dex.enabled=true --set applicationset.enabled=true --set configs.params.server.insecure=false 验证服务状态 # kubectl get pods -n argocd kubectl port-forward svc/argocd-server -n argocd 8080:443 访问 UI: https://localhost:8080\n获取初始 admin 密码 # kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 🧩 四、ApplicationSet Controller 简介 # ApplicationSet 允许你批量创建和动态生成多个 Application，适用于：\n多微服务部署 多环境/多集群 GitOps 自动化生成 YAML 应用资源 常用 Generators 包括：\nGit Generator（根据目录结构生成） List Generator（静态定义多个服务） Cluster Generator（为每个集群生成一个应用） 官方文档：https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/\n🔒 五、启用 SSO（可选，生产建议） # 修改 argocd-cm 和 argocd-secret 配置，启用 OIDC/OAuth2 认证（如 GitHub、Dex、Auth0）。\n完整配置见生产部署文档\ndata: oidc.config: | name: GitHub issuer: https://github.com/login/oauth clientID: YOUR_CLIENT_ID clientSecret: $oidc.github.clientSecret 📣 六、集成 Argo CD Notifications（可选） # kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-notifications/stable/manifests/install.yaml 添加通知通道（Slack/Webhook/Email）并配置 argocd-notifications-cm 和 Secret。\n🛡 七、生产部署建议 # 类别 推荐做法 高可用性 为 controller/server 设置副本数，使用 LoadBalancer 或 Ingress 安全性 启用 SSO，禁用默认 admin，配置 RBAC 多集群支持 注册多个集群，使用 ApplicationSet 进行批量管理 可观测性 启用 Prometheus、Grafana 监控 资源隔离 使用 Argo CD Projects 区分业务/环境 存储与备份 所有 manifests 存入 Git，配置定期备份 🔗 推荐文档 # Argo CD Helm Chart：https://github.com/argoproj/argo-helm/tree/main/charts/argo-cd ApplicationSet 示例：https://github.com/argoproj-labs/applicationset Argo CD Operator Manual：https://argo-cd.readthedocs.io/en/stable/operator-manual/ "},{"id":61,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-event/install/","title":"install","section":"argo event","content":" 🚀 Argo Events 生产环境部署指南 # 本指南介绍如何在 Kubernetes 上以生产级配置部署 Argo Events，结合 NATS 作为 EventBus、Ingress 曝露 Webhook、TLS 安全、以及 Prometheus 监控等增强功能。\n✅ 一、安装前准备 # Kubernetes 1.21+ 集群 已安装 Ingress Controller（如 NGINX） 可选组件：Prometheus、Cert-Manager、Argo Workflows 🧱 二、部署 Argo Events 控制器 + CRD # kubectl create ns argo-events kubectl apply -n argo-events -f https://github.com/argoproj/argo-events/releases/latest/download/install.yaml 这会安装：\nCRDs: EventSource, Sensor, EventBus 控制器 Deployment 📡 三、部署 NATS EventBus（默认推荐） # apiVersion: argoproj.io/v1alpha1 kind: EventBus metadata: name: default namespace: argo-events spec: nats: native: replicas: 3 auth: token containerTemplate: resources: requests: cpu: 100m memory: 128Mi 创建命令：\nkubectl apply -f eventbus-nats.yaml 🌐 四、暴露 Webhook Source（使用 Ingress + TLS） # EventSource 示例：Webhook # apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: webhook-source namespace: argo-events spec: service: ports: - port: 12000 targetPort: 12000 webhook: push: endpoint: /webhook method: POST port: 12000 配置 Ingress # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: events-webhook namespace: argo-events annotations: cert-manager.io/cluster-issuer: letsencrypt-prod spec: tls: - hosts: [events.example.com] secretName: webhook-cert rules: - host: events.example.com http: paths: - path: /webhook pathType: Prefix backend: service: name: webhook-source-eventsource-svc port: number: 12000 🔐 五、安全与认证建议 # 类别 建议 接口认证 使用网关或 Ingress 层的 Basic Auth 或 JWT 验证 TLS 加密 使用 Cert-Manager 自动签发证书 Secret 加密 使用 Sealed Secrets 或 External Secret 管理器 RBAC 最小权限 限制 Sensor 控制器的角色为最小可运行权限 📈 六、监控与可观测性 # 启用 metrics # 所有 Argo Events 控制器默认支持 Prometheus metrics，可通过如下配置暴露端口：\nspec: containers: - name: sensor-controller ports: - name: metrics containerPort: 8080 Prometheus 配置抓取 # scrape_configs: - job_name: \u0026#39;argo-events\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_label_app] regex: argo-events action: keep 🔄 七、自动触发 Argo Workflows 示例（Sensor） # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: webhook-sensor namespace: argo-events spec: dependencies: - name: push eventSourceName: webhook-source eventName: push triggers: - template: name: start-workflow k8s: operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: triggered- spec: entrypoint: hello templates: - name: hello container: image: alpine command: [echo] args: [\u0026#34;Hello Argo Events\u0026#34;] ✅ 总结部署要点 # 组件 是否必须 说明 EventSource/Sensor Controller ✅ 是 事件处理核心 EventBus（NATS） ✅ 是 默认消息通道 Ingress + Cert-Manager ✅ 推荐 Webhook 安全接入 Prometheus ⛔ 可选 可观测性增强 Argo Workflows ⛔ 可选 如果使用 Events 触发 Workflow 📚 推荐资源 # Argo Events 官网：https://argoproj.github.io/argo-events/ EventBus 类型：https://argoproj.github.io/argo-events/eventbus/ 示例合集：https://github.com/argoproj/argo-events/tree/master/examples "},{"id":62,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-rollout/install/","title":"install","section":"argo rollout","content":" 🚀 Argo Rollouts 生产环境部署指南 # 本指南介绍如何在生产环境中部署 Argo Rollouts，集成监控系统，实现金丝雀发布、蓝绿部署、自动分析等高级发布策略。\n✅ 一、组件功能概览 # 组件 说明 Rollouts Controller 替代原生 Deployment 控制器，实现高级发布策略 AnalysisRun 进行 Prometheus 等指标分析 Experiment 支持 AB 测试对比部署效果 Argo Rollouts Kubectl Plugin 用于 CLI 管理 Rollout 状态 Dashboard UI（可选） 与 Argo CD 集成的 Rollouts 插件界面 🧱 二、部署 Rollouts Controller # 方法一：快速安装（官方 YAML） # kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 方法二：使用 Helm（推荐生产） # helm repo add argo https://argoproj.github.io/argo-helm helm repo update helm install argo-rollouts argo/argo-rollouts --namespace argo-rollouts --create-namespace --set controller.replicaCount=2 --set metrics.enabled=true --set dashboard.enabled=true 🔧 三、配置 Prometheus 集成（指标分析支持） # Rollouts 默认支持将 AnalysisRun 数据输出到 Prometheus，示例配置：\nanalysis: templates: - name: success-rate-check metrics: - name: error-rate provider: prometheus: address: http://prometheus.monitoring.svc.cluster.local:9090 query: rate(http_requests_total{status=~\u0026#34;5..\u0026#34;}[1m]) Prometheus 地址需要与你集群中实际部署的服务匹配。\n🎨 四、与 Argo CD 集成 UI # 如果你使用 Argo CD，并启用了 Rollouts Dashboard 插件，可以在应用中看到 Rollout 状态图：\n--set dashboard.enabled=true # 在 Helm 安装时启用 UI 可展示：当前阶段、金丝雀比例、活动服务/预览服务等信息。\n🛡️ 五、生产部署建议 # 分类 建议 高可用性 controller 设置为多副本，使用 readiness/liveness probe 安全性 限制 namespace 访问权限，审计 AnalysisRun 输出 日志分析 配合 Loki/EFK 收集 controller 日志 指标分析 强烈建议配合 Prometheus/Datadog 流量控制 建议使用 Istio/Nginx 服务网关进行金丝雀流量控制 自动化 可结合 GitOps（Argo CD）管理 Rollout YAML Rollback 支持 启用 automatic rollback 条件判断策略 🔄 六、金丝雀/蓝绿发布策略示例（基础） # 金丝雀发布策略 # strategy: canary: steps: - setWeight: 10 - pause: {duration: 2m} - setWeight: 50 - pause: {} analysis: templates: - templateName: success-rate-check 蓝绿部署策略 # strategy: blueGreen: activeService: app-service-active previewService: app-service-preview autoPromotionEnabled: false 📚 推荐阅读 # 官方文档：https://argo-rollouts.readthedocs.io/ 指标分析配置：https://argo-rollouts.readthedocs.io/en/stable/features/analysis/ Prometheus 监控接入：https://argo-rollouts.readthedocs.io/en/stable/features/metrics/ "},{"id":63,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-workflow/install/","title":"install","section":"argo workflow","content":" 📦 Argo Workflows 安装与启动参数详解 # ✅ 安装 Argo Workflows # 方法一：官方快速安装 # kubectl create namespace argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/install.yaml kubectl -n argo port-forward deployment/argo-server 2746:2746 方法二：使用 Helm 安装（推荐） # helm repo add argo https://argoproj.github.io/argo-helm helm repo update helm install argo-workflows argo/argo-workflows --namespace argo --create-namespace --set server.secure=true --set controller.workflowNamespaces=\u0026#34;{argo}\u0026#34; --set useDefaultArtifactRepo=true ⚙️ argo-server 常见启动参数说明 # 参数 作用 --secure 是否启用 TLS（建议开启） --auth-mode 认证模式，可选：server, client, sso, hybrid --configmap 配置 Workflow Controller 的配置来源 --namespaced 是否启用命名空间隔离（每个用户只能看自己 namespace 的 workflow） --basehref 设置 UI 路由前缀（如部署在子路径下） --namespace 设置默认的 namespace --loglevel 设置日志级别（例如 info, debug） 🔐 启用 SSO（Single Sign-On） # Argo Workflows 支持基于 OIDC 的 SSO 认证。以下为基本配置步骤：\n步骤一：配置 argo-server 使用 SSO 模式 # --auth-mode sso --configmap workflow-controller-configmap 步骤二：创建 SSO 配置（ConfigMap） # apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap namespace: argo data: sso: | issuer: https://accounts.google.com clientId: my-client-id clientSecret: $SSO_CLIENT_SECRET redirectUrl: https://argo.example.com/oauth2/callback scopes: - email - profile rbac: enabled: true defaultPolicy: \u0026#39;role:readonly\u0026#39; scopes: [\u0026#39;groups\u0026#39;] 注意：clientId 与 clientSecret 来自你的 OIDC 提供商（如 Google、Auth0、Dex）\n💾 数据持久化配置（连接数据库） # Argo Workflows 默认使用 Kubernetes 的状态进行调度，但也可集成 PostgreSQL 以提升高可用和性能（通常用于 Argo Server + Workflow Archive）。\n步骤一：配置 PostgreSQL 信息 # controller: persistence: archive: true postgresql: host: \u0026lt;postgres-host\u0026gt; port: 5432 database: argo tableName: argo_workflows userName: argo passwordSecret: name: argo-postgres-secret key: password 步骤二：创建密钥 # apiVersion: v1 kind: Secret metadata: name: argo-postgres-secret namespace: argo type: Opaque data: password: \u0026lt;base64-encoded-password\u0026gt; 🔍 验证部署 # kubectl get pods -n argo kubectl logs deployment/argo-server -n argo kubectl get workflow -n argo 访问 UI: http://localhost:2746\n📚 更多参考 # 官方 Helm Chart 文档: https://github.com/argoproj/argo-helm/tree/main/charts/argo-workflows SSO 文档: https://argoproj.github.io/argo-workflows/argo-server-sso/ Artifact Persistence: https://argoproj.github.io/argo-workflows/workflow-archive/ "},{"id":64,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/backstage/install/","title":"install","section":"Backstage","content":" Backstage Helm 安装指南（生产环境） # 本文档介绍如何使用 Helm 在 Kubernetes 中部署生产级别的 Backstage 开发者门户。\n🧰 前置条件 # 确保以下组件已安装并配置：\nKubernetes 集群（版本 ≥ 1.21） Helm 3.x Docker（构建镜像用） 可访问的容器镜像仓库 PostgreSQL 数据库（可选，Helm 可自动安装） 🚀 步骤一：构建并推送 Backstage 镜像 # # 初始化项目（如未创建） npx @backstage/create-app cd my-backstage-app # 构建生产镜像 docker build -t my-org/backstage:latest . docker push my-org/backstage:latest 🔧 步骤二：添加 Helm 仓库 # helm repo add backstage https://backstage.github.io/charts helm repo update 📝 步骤三：创建 values.yaml 配置文件 # 以下是一个简化的生产配置：\nimage: repository: my-org/backstage tag: latest pullPolicy: IfNotPresent replicaCount: 2 ingress: enabled: true host: backstage.example.com annotations: nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; tls: - hosts: - backstage.example.com secretName: backstage-tls postgresql: enabled: true postgresqlPassword: strong-password appConfig: app: baseUrl: https://backstage.example.com backend: baseUrl: https://backstage.example.com cors: origin: https://backstage.example.com database: client: pg connection: host: postgres user: postgres password: strong-password database: backstage ✅ 推荐使用自定义域名 + HTTPS + TLS。\n🛠 步骤四：安装 Backstage # helm install backstage backstage/backstage -n backstage --create-namespace -f values.yaml 🔁 步骤五：更新部署（如有变更） # helm upgrade backstage backstage/backstage -n backstage -f values.yaml 🧪 步骤六：验证部署 # kubectl get pods -n backstage kubectl get svc -n backstage 访问：https://backstage.example.com\n📌 附加建议 # 配合 cert-manager 自动生成 TLS 证书 使用 External PostgreSQL 实现持久化和高可用 使用 Fluent Bit 收集日志，配合 Loki 或 ELK 📚 参考链接 # https://backstage.io/docs https://github.com/backstage/charts "},{"id":65,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/certmanager/install/","title":"install","section":"CertManager","content":" cert-manager 生产环境部署指南 # cert-manager 生产环境部署指南 # 本文档介绍如何在 Kubernetes 生产环境中部署 cert-manager，并确保其安全、稳定、高可用运行。\n一、准备工作 # 已部署 Kubernetes 集群（版本建议 \u0026gt;= 1.19） 集群中已安装 kubectl、具备管理员权限 推荐为 cert-manager 创建独立的命名空间 网络能访问目标证书颁发机构（如 Let\u0026rsquo;s Encrypt） 二、部署步骤 # 1. 创建命名空间 # kubectl create namespace cert-manager 2. 添加 Jetstack Helm 仓库 # helm repo add jetstack https://charts.jetstack.io helm repo update 3. 安装 cert-manager（使用 CRDs） # helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.14.3 \\ --set installCRDs=true ✅ 说明：请根据实际需要调整版本号。\n4. 验证部署 # kubectl get pods -n cert-manager 应看到如下组件正在运行：\ncert-manager cert-manager-webhook cert-manager-cainjector 5. 创建 ClusterIssuer 示例（以 Let\u0026rsquo;s Encrypt 为例） # apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: your-email@example.com privateKeySecretRef: name: letsencrypt-prod-account-key solvers: - http01: ingress: class: nginx kubectl apply -f clusterissuer.yaml 6. 创建 Certificate 示例（自动签发证书） # apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: my-tls-cert namespace: default spec: secretName: my-tls-cert-secret issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: example.com dnsNames: - example.com - www.example.com 7. 配合 Ingress 使用 # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: cert-manager.io/cluster-issuer: letsencrypt-prod spec: tls: - hosts: - example.com secretName: my-tls-cert-secret rules: - host: example.com http: paths: - path: / pathType: Prefix backend: service: name: example-service port: number: 80 三、安全与高可用建议 # 为 cert-manager 配置资源限制和 pod disruption budget 将证书 Secret 配置备份策略（如 Velero） 监控证书状态，结合 Prometheus/Grafana 设置告警 使用 DNS-01 验证适应多 ingress controller 和私有网络环境 建议结合 ExternalDNS 和自动化 DNS 提供商（如 Cloudflare） 四、常见问题排查 # 问题 解决方案 Challenge 一直 pending 检查 DNS 或 Ingress 配置，确保验证路径可达 secret 未创建 确保 Issuer 正确配置，邮箱正确，ACME 服务可访问 证书未续签 检查 cert-manager 日志，确认 webhook、cainjector 正常运行 五、参考资料 # cert-manager 官方文档 Helm 安装参考 Let\u0026rsquo;s Encrypt 使用指南 "},{"id":66,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/cilium/install/","title":"install","section":"Cilium","content":" 🏗️ 在生产环境安装 Cilium 并集成 Envoy # 一、生产环境安装 Cilium # 1️⃣ 环境准备 # Kubernetes v1.23+（建议 v1.25+） Linux 内核版本 \u0026gt;= 5.10（支持高级 eBPF 特性） 已安装 Helm、kubectl、containerd 或其他 CRI 2️⃣ 添加 Cilium Helm 仓库 # helm repo add cilium https://helm.cilium.io/ helm repo update 3️⃣ 安装 Cilium（推荐开启 kube-proxy 替代） # helm install cilium cilium/cilium \\ --version 1.14.4 \\ --namespace kube-system \\ --set kubeProxyReplacement=strict \\ --set k8sServiceHost=\u0026lt;API_SERVER_HOST\u0026gt; \\ --set k8sServicePort=6443 \\ --set enableHubble=true \\ --set hubble.relay.enabled=true \\ --set hubble.ui.enabled=true 注意：替换 \u0026lt;API_SERVER_HOST\u0026gt; 为你的 Kubernetes API Server 地址。\n4️⃣ 安装 CLI 工具（可选） # cilium status cilium connectivity test 二、集成 Envoy 实现 L7 策略或流量代理 # 📌 Cilium 中的 Envoy 用法 # Cilium 内部集成了 Envoy 作为透明代理，配合 eBPF socket hooks 实现 L7 网络策略与应用协议过滤（如 HTTP、gRPC、Kafka）。你无需手动部署 Envoy Sidecar。\n✅ 启用 L7 HTTP 策略功能 # Cilium 默认启用了内置 Envoy，用于处理 L7 策略。你只需部署 CiliumNetworkPolicy 即可实现应用层控制。\n示例策略：仅允许 GET 请求 /api # apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: l7-http-policy spec: endpointSelector: matchLabels: app: frontend ingress: - fromEndpoints: - matchLabels: app: backend toPorts: - ports: - port: \u0026#34;80\u0026#34; protocol: TCP rules: http: - method: GET path: \u0026#34;/api\u0026#34; Cilium 会自动在需要时为目标 Pod 启动 Envoy 实例，作为 L7 数据流的过滤器。\n三、验证与调试 # 查看 Envoy 状态 # cilium envoy config \u0026lt;pod-name\u0026gt; cilium monitor --type l7 查看可视化界面（Hubble） # kubectl port-forward -n kube-system svc/hubble-ui 12000:80 # 打开浏览器 http://localhost:12000 四、总结 # 在生产环境中部署 Cilium 时推荐启用 kube-proxy 替代（strict 模式）以提升性能 Cilium 原生集成 Envoy，无需单独部署 Sidecar 即可实现 L7 策略控制 搭配 Hubble 可实现端到端网络可视化、HTTP/gRPC 请求监控 Cilium 提供了轻量、高性能的 Service Mesh 替代方案，适合对安全、观测、性能有高要求的生产集群环境。\n"},{"id":67,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/questions/","title":"questions","section":"Kubernetes","content":"本文介绍kubernetes 常见的问题\n"},{"id":68,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/prometheus/questions/","title":"questions","section":"prometheus","content":"本人介绍prometheus 常见的问题\n"},{"id":69,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/develop/client-go/informer/resync/","title":"Resync","section":"informer","content":" ReSync # "},{"id":70,"href":"/docs/go/sync/","title":"sync","section":"Go","content":"本章节介绍 sync 包的使用\n"},{"id":71,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/metrics/thanos/","title":"thanos","section":"metrics","content":" "},{"id":72,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%9B%91%E6%8E%A7/trace/","title":"trace","section":"监控","content":"🔹 Trace 的工作原理\n1️⃣ 请求进入系统 •\t入口（如 API Gateway）生成一个 Trace ID •\t记录 起始时间\n2️⃣ 请求传播 •\t每个微服务 创建一个 Span •\t传递 Trace ID（上下文） •\t记录 时间、请求参数、响应时间\n3️⃣ 请求完成 •\t计算 每个服务的执行时间 •\t记录 错误信息 •\t上报 Trace 数据到 存储 \u0026amp; 可视化系统\n🔹 Trace 数据结构\n一个 Trace 由多个 Span 组成，每个 Span 记录：\n{ \u0026#34;trace_id\u0026#34;: \u0026#34;abcd-1234\u0026#34;, \u0026#34;span_id\u0026#34;: \u0026#34;efgh-5678\u0026#34;, \u0026#34;parent_id\u0026#34;: \u0026#34;ijkl-9101\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;user-service\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;GET /user/info\u0026#34;, \u0026#34;start_time\u0026#34;: 1710000000000, \u0026#34;duration_ms\u0026#34;: 30, \u0026#34;tags\u0026#34;: { \u0026#34;http.status_code\u0026#34;: 200, \u0026#34;db.query\u0026#34;: \u0026#34;SELECT * FROM users\u0026#34; } } 🚀 总结\n✅ Trace 适用于分布式微服务，帮助分析慢请求、调用链\n✅ 核心概念：Trace、Span、上下文传播、存储 \u0026amp; 可视化\n✅ Jaeger、OpenTelemetry、Tempo 是常见的 Trace 方案\n✅ 相比传统日志，Trace 更适合故障排查 \u0026amp; 性能优化\n"},{"id":73,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%9B%9E%E6%BA%AF/","title":"回溯","section":"算法","content":"本章节介绍回溯方法\n基本思想 # 尝试构建解（探索） 如果不满足条件，则回退（回溯） 如果满足条件，则记录解 步骤 # 选择（Choose）：尝试做出选择，向解空间深处探索。 约束（Constraint）：检查当前选择是否满足问题约束条件，若不满足则回退。 撤销（Unchoose）：如果该选择导致后续无解，则撤销选择，回到上一步。 优缺点 # 优点\n易于实现：代码结构清晰，递归思维自然适配回溯法。 能求所有解：适用于组合、排列、子集问题，能遍历所有可能方案。 适用于树形结构问题：如决策树、DFS 搜索路径。 可以结合剪枝优化：如 used[] 记录状态、提前终止无效搜索。 缺点 时间复杂度高： 组合：O(C(n, k)) ≈ 指数级 排列：O(n!)（非常高） 迷宫：最坏 O(4^n)（四个方向尝试） 重复计算： 例如编辑距离计算，回溯会重复计算子问题，导致超时。 优化：记忆化搜索 + 剪枝。 不适合大规模数据： 搜索空间太大时，直接超时！ 需要优化，如 剪枝、DP 代替回溯。 "},{"id":74,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-rollout/","title":"argo rollout","section":"Argo","content":"介绍使用argo rollout\n"},{"id":75,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/kubelet/cni/cilium/","title":"cilium","section":"CNI","content":"介绍使用cilium作为kubernetes CNI网络插件的使用\n"},{"id":76,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/kubernetes/operate/kubelet/cni/","title":"CNI","section":"operate","content":"介绍使用cilium作为kubernetes CNI网络插件的使用\n"},{"id":77,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-cd/example/","title":"example","section":"argo cd","content":" 🚀 Argo CD 多集群、多环境、Helm 管理 + Project 隔离示例（生产级复杂实践） # 本示例展示如何使用 Argo CD + ApplicationSet 管理多个集群、多环境、多服务，结合 Helm、Argo CD Project 和 RBAC，构建生产级 GitOps 平台。\n📁 Git 仓库结构假设 # git-repo/ └── services/ ├── svc-a/ │ └── charts/ │ ├── dev/values.yaml │ ├── stage/values.yaml │ └── prod/values.yaml ├── svc-b/ │ └── charts/ │ ├── dev/values.yaml │ └── prod/values.yaml 🧩 1. 创建 Argo CD Project（隔离服务与环境） # apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: svc-a namespace: argocd spec: description: svc-a service deployments sourceRepos: - https://github.com/your-org/your-repo.git destinations: - namespace: svc-a-* server: https://kubernetes.default.svc - namespace: svc-a-* server: https://k8s-stage.example.com - namespace: svc-a-* server: https://k8s-prod.example.com clusterResourceWhitelist: - group: \u0026#34;*\u0026#34; kind: \u0026#34;*\u0026#34; namespaceResourceWhitelist: - group: \u0026#34;*\u0026#34; kind: \u0026#34;*\u0026#34; ⚙️ 2. ApplicationSet 示例（支持多环境 + 多集群 + Helm + Project） # apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: svc-a-apps namespace: argocd spec: generators: - matrix: generators: - git: repoURL: https://github.com/your-org/your-repo.git revision: main directories: - path: services/svc-a/charts/* - list: elements: - env: dev cluster: https://kubernetes.default.svc - env: stage cluster: https://k8s-stage.example.com - env: prod cluster: https://k8s-prod.example.com template: metadata: name: svc-a-{{env}} spec: project: svc-a source: repoURL: https://github.com/your-org/your-repo.git targetRevision: main path: services/svc-a/charts/{{env}} helm: valueFiles: - values.yaml destination: server: \u0026#39;{{cluster}}\u0026#39; namespace: svc-a-{{env}} syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true 🔒 3. RBAC 示例（可选） # 为特定用户组限制访问 svc-a 项目与环境：\napiVersion: v1 kind: ConfigMap metadata: name: argocd-rbac-cm namespace: argocd data: policy.csv: | g, dev-team, role:svc-a-dev p, role:svc-a-dev, applications, get, svc-a-*, allow p, role:svc-a-dev, projects, get, svc-a, allow ✅ 高级功能回顾 # 功能 描述 Project 资源隔离 每个服务独立 Project，可设置独立权限 多集群部署 根据环境匹配不同 Kubernetes API server 多环境支持 使用 Helm 的子目录 values 管理 dev/stage/prod Matrix Generator 支持服务 + 环境组合生成多个应用 自动同步与自愈 automated + selfHeal 提升可靠性 命名空间自动创建 支持 CreateNamespace=true RBAC 控制 精细化控制用户访问权限 📦 生产部署注意事项 # 集群需提前通过 argocd cluster add 添加并设置 RBAC 每个服务建议用独立 Project 管理 Helm charts 存放路径应一致规范 使用专有 Git 分支管理环境（可扩展 Git Generator 条件） 结合通知系统监控同步失败状态 🔗 推荐资源 # Project 配置文档：https://argo-cd.readthedocs.io/en/stable/user-guide/projects/ Matrix Generator 文档：https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/MATRIX-Generator/ Helm 支持说明：https://argo-cd.readthedocs.io/en/stable/user-guide/helm/ "},{"id":78,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-workflow/example/","title":"example","section":"argo workflow","content":" 🚀 最复杂的 Argo Workflow 示例 # 本示例涵盖 Argo Workflow 的核心高级功能，包括：\nDAG + 步骤式混用 参数传递与输出 Artifact 使用 Script 模板 Sidecar 容器 条件执行（when） 重试策略 并发控制 TTL 策略 持久化存储（Artifact S3 示例） 手动暂停与继续 工作流模板复用（WorkflowTemplate） 🧾 Workflow YAML 示例 # apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: complex-workflow- spec: entrypoint: main-dag arguments: parameters: - name: env value: \u0026#34;prod\u0026#34; ttlStrategy: secondsAfterSuccess: 3600 parallelism: 2 templates: - name: main-dag dag: tasks: - name: generate template: generate-data - name: process dependencies: [generate] template: process-data arguments: artifacts: - name: input-data from: \u0026#34;{{tasks.generate.outputs.artifacts.out-data}}\u0026#34; - name: conditional-step dependencies: [generate] template: conditional-script when: \u0026#34;{{inputs.parameters.env}} == prod\u0026#34; - name: deploy dependencies: [process, conditional-step] template: deploy-app - name: generate-data outputs: artifacts: - name: out-data path: /tmp/output.txt container: image: alpine command: [sh, -c] args: [\u0026#34;echo \u0026#39;data-content\u0026#39; \u0026gt; /tmp/output.txt\u0026#34;] - name: process-data inputs: artifacts: - name: input-data path: /tmp/input.txt container: image: python:3.8 command: [python] args: - -c - | with open(\u0026#39;/tmp/input.txt\u0026#39;) as f: data = f.read() print(f\u0026#34;Processed: {data}\u0026#34;) - name: conditional-script script: image: bash:5.1 command: [bash] source: | echo \u0026#34;Running only in production\u0026#34; - name: deploy-app container: image: curlimages/curl command: [\u0026#34;curl\u0026#34;] args: [\u0026#34;-X\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;http://example.com/deploy\u0026#34;] - name: sidecar-demo container: image: busybox command: [sh, -c] args: [\u0026#34;echo main container \u0026amp;\u0026amp; sleep 10\u0026#34;] sidecars: - name: logger image: busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;while true; do echo sidecar running; sleep 2; done\u0026#34;] mirrorVolumeMounts: true - name: retry-step retryStrategy: limit: 3 retryPolicy: \u0026#34;Always\u0026#34; container: image: alpine command: [sh, -c] args: [\u0026#34;exit 1\u0026#34;] - name: approval-step suspend: {} 🧠 功能说明对照表 # 功能模块 说明 DAG 模式 使用 main-dag 模板构建任务依赖 Artifact 任务间通过 /tmp/output.txt 文件传递数据 参数传递 通过 arguments.parameters 控制部署环境 条件执行 只有在 env == prod 时才运行 conditional-script Script 模板 内联 bash 脚本 Sidecar 容器 sidecar-demo 展示日志采集 Sidecar 重试策略 retry-step 会失败并最多重试 3 次 Suspend 手动批准 approval-step 模拟人工审核点 TTL 策略 完成 3600 秒后自动清理 并发限制 parallelism: 2 限制并发任务数为 2 📦 要求的环境依赖 # 已安装 Argo Workflows Controller \u0026amp; Server Artifact 存储（如 MinIO、S3）已配置（示例略去具体 config） 如果使用 WorkflowTemplate，可拆分 generate, process, deploy 为可复用模板 📌 小提示 # 可以通过 Argo UI 或 CLI 提交此 Workflow 若需手动 Resume suspend 任务： argo resume \u0026lt;workflow-name\u0026gt; 🔗 相关参考 # Argo 官方文档：https://argoproj.github.io/argo-workflows/ 示例仓库：https://github.com/argoproj/argo-workflows/tree/master/examples "},{"id":79,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/backstage/example/","title":"example","section":"Backstage","content":" Backstage 生产环境使用示例（复杂用例） # 本文展示一个包含多个插件、权限控制、自定义服务目录、CI/CD 集成、文档生成和 Kubernetes 可观测性的复杂 Backstage 使用场景。\n🧩 系统架构图示意 # 开发者 --\u0026gt; Backstage Portal --\u0026gt; 插件系统 | +--\u0026gt; Catalog (服务注册) +--\u0026gt; TechDocs (文档平台) +--\u0026gt; ArgoCD Plugin (CI/CD 状态) +--\u0026gt; Kubernetes Plugin (集群状态) +--\u0026gt; Auth (GitHub OAuth) +--\u0026gt; External Monitoring (Prometheus/Grafana) 🗃 软件目录配置（catalog-info.yaml） # apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: payment-service description: 支付服务，负责处理所有支付相关逻辑 annotations: github.com/project-slug: myorg/payment-service backstage.io/techdocs-ref: dir:. argocd/app-name: payment-service prometheus.io/rule: payment-alerts spec: type: service lifecycle: production owner: team-payments system: payments-platform dependsOn: - component:default/database 🔒 身份认证与权限控制 # 使用 GitHub OAuth + RBAC 策略：\nauth: providers: github: production: clientId: \u0026lt;client-id\u0026gt; clientSecret: \u0026lt;client-secret\u0026gt; enterpriseInstanceUrl: https://github.myorg.com permission: enabled: true policies: - policy: allow resourceType: component actions: [\u0026#39;read\u0026#39;] subjects: - group: developers - policy: deny resourceType: component actions: [\u0026#39;delete\u0026#39;] 🚀 CI/CD 可视化（ArgoCD 插件） # 在 app-config.yaml 中配置：\nargocd: appLocatorMethods: - type: config instances: - name: main url: https://argocd.example.com token: ${ARGOCD_TOKEN} 前端插件可展示 Argo 应用部署状态。\n📄 技术文档（TechDocs） # 项目结构：\n. ├── docs │ └── index.md ├── mkdocs.yml └── catalog-info.yaml 示例 mkdocs.yml:\nsite_name: Payment Service Docs nav: - Home: index.md plugins: - techdocs-core 构建并部署 TechDocs：\nyarn techdocs:build yarn techdocs:publish 🛰 Kubernetes 插件配置 # 配置多个集群的访问凭证：\nkubernetes: clusterLocatorMethods: - type: config clusters: - name: prod-cluster url: https://k8s-prod-api.example.com authProvider: serviceAccount serviceAccountToken: ${K8S_TOKEN} skipTLSVerify: false 可视化展示 Pod、Service、Deployment 状态。\n📊 监控系统集成（Prometheus + Grafana） # Prometheus 配置：\nprometheus: instances: - name: main url: http://prometheus.monitoring.svc:9090 Grafana：\n可通过 iframe 嵌入 Grafana 仪表盘至自定义插件或页面。\n🎯 结语 # 通过 Backstage 的插件化架构，可以将 DevOps 工具链整合进一个统一平台，提升开发者效率。此示例展示了一个集成 ArgoCD、Kubernetes、Prometheus、TechDocs 等复杂插件的生产环境用例。\n📚 参考链接 # https://backstage.io/docs https://github.com/backstage/backstage "},{"id":80,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/certmanager/example/","title":"example","section":"CertManager","content":" cert-manager 复杂生产环境使用示例 # cert-manager 复杂生产环境使用示例 # 本示例展示了如何在生产环境中使用 cert-manager 实现以下目标：\n使用 Let\u0026rsquo;s Encrypt 作为证书颁发机构 使用 DNS-01 验证（支持多 ingress controller 和私有环境） 将证书应用于 Ingress（支持多域名） 自动续期与 Secret 管理 多租户支持（ClusterIssuer + 命名空间 Certificate） 一、前提条件 # Kubernetes 集群已部署 cert-manager DNS 提供商支持 API 控制（如 Cloudflare、AliDNS、Route53） 集群可访问 DNS API（配置 Secret 以供认证） 二、配置 Cloudflare DNS API Secret # apiVersion: v1 kind: Secret metadata: name: cloudflare-api-token-secret namespace: cert-manager type: Opaque stringData: api-token: CLOUDFLARE_API_TOKEN 请将 CLOUDFLARE_API_TOKEN 替换为实际 token。建议使用 read/write 权限控制子域名。\n三、创建 ClusterIssuer 使用 DNS-01 验证 # apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-dns spec: acme: email: your-email@example.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-dns-account-key solvers: - dns01: cloudflare: email: your-email@example.com apiTokenSecretRef: name: cloudflare-api-token-secret key: api-token 四、创建生产 Certificate 对象（多域名 + 自定义 DNS） # apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: multi-domain-cert namespace: production spec: secretName: tls-multi-domain duration: 2160h # 90天 renewBefore: 360h # 15天前续期 issuerRef: name: letsencrypt-dns kind: ClusterIssuer commonName: app.example.com dnsNames: - app.example.com - api.example.com - dashboard.example.com 五、配置支持 HTTPS 的 Ingress # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress namespace: production annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-dns spec: tls: - hosts: - app.example.com - api.example.com - dashboard.example.com secretName: tls-multi-domain rules: - host: app.example.com http: paths: - path: / pathType: Prefix backend: service: name: app-service port: number: 80 - host: api.example.com http: paths: - path: / pathType: Prefix backend: service: name: api-service port: number: 80 - host: dashboard.example.com http: paths: - path: / pathType: Prefix backend: service: name: dashboard-service port: number: 80 六、建议与注意事项 # 推荐为每个租户或业务线使用独立的 Certificate + Secret 可使用 ExternalDNS 实现自动化 DNS 管理 配合 Prometheus 监控证书剩余有效期（cert-manager 提供 metrics） 保持 cert-manager Controller、webhook 和 cainjector 高可用部署 七、故障排查 # kubectl describe certificate multi-domain-cert -n production kubectl logs -l app=cert-manager -n cert-manager 八、参考文档 # cert-manager DNS-01 官方文档 Cloudflare API Token 权限配置 "},{"id":81,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/cilium/example/","title":"example","section":"Cilium","content":" 🧠 Cilium 生产环境复杂示例（L7 策略 + Egress Gateway + ClusterMesh） # 本示例模拟一个典型的生产微服务架构场景，包含以下特性：\n使用 Cilium 作为 CNI 基于标签的 L3/L4 策略控制 精细的 L7 HTTP 策略（基于路径、方法） Egress Gateway 限定 Pod 出口 IP 跨命名空间策略 多集群通信（ClusterMesh 简要配置） 📦 应用拓扑说明 # frontend (namespace: frontend) | |--\u0026gt; backend (namespace: backend) | |--\u0026gt; external API (egress: api.external.com) 📑 1. 跨命名空间的 L7 策略 # apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: allow-frontend-to-backend namespace: backend spec: endpointSelector: matchLabels: app: backend ingress: - fromEndpoints: - matchLabels: app: frontend - namespaceSelector: matchLabels: name: frontend toPorts: - ports: - port: \u0026#34;8080\u0026#34; protocol: TCP rules: http: - method: \u0026#34;GET\u0026#34; path: \u0026#34;/api/v1/status\u0026#34; - method: \u0026#34;POST\u0026#34; path: \u0026#34;/api/v1/submit\u0026#34; 🌐 2. Egress Gateway 配置（出公网使用固定 IP） # Egress Gateway Policy # apiVersion: cilium.io/v2alpha1 kind: CiliumEgressGatewayPolicy metadata: name: backend-egress spec: egress: - podSelector: matchLabels: app: backend namespaceSelector: matchLabels: name: backend destinationCIDRs: - \u0026#34;0.0.0.0/0\u0026#34; egressGateway: nodeSelector: matchLabels: role: egress-gw 对应节点需设置标识：kubectl label node \u0026lt;node\u0026gt; role=egress-gw\n🔐 3. DNS-based 策略（只允许访问特定域名） # apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: dns-egress-policy namespace: backend spec: endpointSelector: matchLabels: app: backend egress: - toFQDNs: - matchName: \u0026#34;api.external.com\u0026#34; ☁️ 4. ClusterMesh 基础配置步骤（简略） # # 在两个集群中都安装 Cilium 且开启 ClusterMesh helm upgrade cilium cilium/cilium --namespace kube-system --set cluster.name=cluster-a --set cluster.id=1 --set clustermesh.useAPIServer=true --set etcd.enabled=true 使用共享 DNS 或 CoreDNS + etcd 连接两个集群 服务和身份会在两个集群间同步 ✅ 5. 验证策略 # cilium status cilium monitor --type l7 cilium connectivity test kubectl exec \u0026lt;frontend-pod\u0026gt; -- curl -X GET backend.backend.svc.cluster.local:8080/api/v1/status 🧾 总结 # 此示例演示了 Cilium 在实际生产中如何组合多个功能：\n精确的应用层访问控制（HTTP Path/Method） 命名空间隔离 外部服务访问控制 Egress IP 管理 多集群服务互通（ClusterMesh） Cilium 可替代多种传统组件，提供统一的、高性能的网络 + 安全 + 可观测平台。\n"},{"id":82,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/keycloak/","title":"Keycloak","section":"云原生","content":"介绍使用Keycloak + OIDC\n"},{"id":83,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%8F%8C%E6%8C%87%E9%92%88/","title":"双指针","section":"算法","content":"本章节介绍双指针\n基本思想 # 双指针（Two Pointers） 是一种常用的算法优化技巧，通过两个指针（索引）在数组或链表上进行遍历，减少不必要的重复计算，提高效率。 适用于 数组、字符串、链表等线性结构，\n查找 比较 去重 排序等。 适用条件 # 有序数组/字符串/链表，方便利用 左右对撞 或 快慢遍历。 寻找特定组合（如和为 target），可以通过 对撞指针 避免暴力解法。 需要动态维护窗口（如最长无重复子串），滑动窗口可高效解决问题。 步骤 # 问题分解：将问题分为若干个子问题 局部最优选择：对每个子问题，选择一个局部最优解 合并解：将局部最优解合并为全局解 经典应用 # 活动选择问题：选择最多的互不重叠的活动。 霍夫曼编码：构造最优前缀编码。 最小生成树：如Prim算法和Kruskal算法。 最短路径问题：如Dijkstra算法。 背包问题的分数版本：选择单位价值最高的物品。 优缺点 # 优点\n降低时间复杂度 套循环，降低 O(n²) 为 O(n)，减少不必要的计算。 适用于有序结构 有序数组、单调递增序列 可用双指针 高效查找目标值。 空间复杂度低 一般只需 O(1) 额外空间，避免额外数组或哈希表。 缺点\n不适用于无序数组 例如 无序数组求和 不能直接用 对撞指针。 有些问题无法规避嵌套循环 例如 四数之和 仍然需要 O(n³)。 复杂情况需要额外逻辑 滑动窗口有时候需要 哈希表 记录状态。 "},{"id":84,"href":"/docs/linux/%E7%BD%91%E7%BB%9C/","title":"网络","section":"linux","content":" 计算机网络 # "},{"id":85,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-event/","title":"argo event","section":"Argo","content":"介绍使用argo event\n"},{"id":86,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/harbor/","title":"Harbor","section":"云原生","content":"本章节介绍Harbor\n"},{"id":87,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%A0%86/","title":"堆","section":"算法","content":"本章节介绍堆\n基本思想 # 堆通常用完全二叉树实现，并存储在数组中，保证高效的插入和删除操作 最大堆：每个节点的值都大于等于其子节点的值，根节点是最大值 最小堆：每个节点的值都小于其子节点的值，根节点是最小值 存储方式 # 使用数组表示完全二叉树 操作 # 插入操作\n把新元素插入到数组末尾 上浮 删除堆顶元素\n用最后一个元素替换堆顶 下沉 堆排序\n构建最大堆 交换根节点和最后一个元素 调整堆 重复上述步骤，直到排序完成 经典应用 # 优先队列（Priority Queue） 使用最小堆实现任务优先级调度 使用最大堆实现Top K 问题 求TOP K 大/小元素 最小堆求TOP K 大元素 最大堆求TOP K 小元素 中位数维护 左边最大堆（维护较小的一半数） 右边最小堆（维护较大的一半数） 中位数为两堆的顶元素 优缺点 # 优点\n插入、删除 操作时间复杂度 O(log n) 动态维护 Top K 问题，适用于大数据场景 优先队列实现简单高效 缺点\n不支持快速查找（不像哈希表 O(1)） 堆排序不是稳定排序（相同元素相对顺序可能改变） 数组实现堆可能需要额外的空间移动 示例 # 最小堆\nimport \u0026#34;container/heap\u0026#34; type MinHeap []int func (h MinHeap) Len() int { return len(h) } func (h MinHeap) Less(i, j int) bool { return h[i] \u0026lt; h[j] } // 最小堆 func (h MinHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h *MinHeap) Push(x any) { *h = append(*h, x.(int)) } func (h *MinHeap) Pop() any { old := *h n := len(old) x := old[n-1] *h = old[:n-1] return x } func main() { h := \u0026amp;MinHeap{} heap.Init(h) heap.Push(h, 5) heap.Push(h, 2) heap.Push(h, 8) heap.Push(h, 1) fmt.Println(heap.Pop(h)) // 1 } "},{"id":88,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/","title":"Argo","section":"云原生","content":"介绍使用argo\n"},{"id":89,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo_e2e_flow_full/","title":"e2e_flow_full","section":"Argo","content":" 🌐 Nginx 全家桶 GitOps – Dev \u0026amp; Prod 金丝雀发布 # 利用 Argo Events → Argo Workflows → Harbor → Argo CD → Argo Rollouts\n实现 代码 Push → Dev Canary → 审批 → Prod Canary 的端到端流水线，\n并通过 Prometheus Analysis 与两级人工审批保障生产安全。 # 🖼️ 架构图 # flowchart TD P[Git Push] --\u0026gt; ES(EventSource\u0026lt;br/\u0026gt;Git Webhook) ES --\u0026gt; EB[EventBus NATS (3)] EB --\u0026gt; S(Sensor) S --\u0026gt; WF[CI Workflow\u0026lt;br/\u0026gt;Kaniko + Helm + 两级审批] WF --\u0026gt; IMG[(Harbor 镜像)] WF --\u0026gt; CH[(Harbor Chart)] WF -.sync-dev.-\u0026gt; CD1[Argo CD Dev\u0026lt;br/\u0026gt;Rollout] WF -.sync-prod.-\u0026gt; CD2[Argo CD Prod\u0026lt;br/\u0026gt;Rollout] CD1 --\u0026gt;|25→100 %| VS1(Istio VS Dev) CD2 --\u0026gt;|10→50→100 %| VS2(Istio VS Prod) Prom[Prometheus] -.metrics.-\u0026gt; VS1 \u0026amp; VS2 📂 目录结构 # nginx-gitops/ ├── Dockerfile ├── chart/ │ ├── Chart.yaml │ ├── values.yaml # 公共 │ ├── values-dev.yaml # Dev │ ├── values-prod.yaml # Prod │ └── templates/ │ ├── rollout.yaml │ └── service.yaml └── argo/ ├── eventbus.yaml ├── eventsource-git.yaml ├── sensor-ci.yaml ├── workflowtemplate-ci-harbor.yaml ├── analysis/success-rate.yaml └── applications/ ├── nginx-dev.yaml └── nginx-prod.yaml 1️⃣ EventBus（NATS） # apiVersion: argoproj.io/v1alpha1 kind: EventBus metadata: name: default namespace: argo-events spec: nats: native: replicas: 3 auth: token 2️⃣ EventSource（Git Webhook） # apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: git-webhook namespace: argo-events spec: service: ports: - port: 12000 targetPort: 12000 github: git-push: endpoint: /push port: 12000 insecure: false secretToken: name: github-secret key: token 3️⃣ Sensor（调用 WorkflowTemplate） # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: ci-sensor namespace: argo-events spec: eventBusName: default dependencies: - name: push eventSourceName: git-webhook eventName: git-push triggers: - template: name: ci-workflow k8s: group: argoproj.io version: v1alpha1 resource: workflows operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: ci-pipeline- spec: workflowTemplateRef: name: ci-template 4️⃣ WorkflowTemplate（CI → 审批 → GitOps） # apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: ci-nginx-harbor namespace: argo spec: entrypoint: pipeline serviceAccountName: argo-ci templates: - name: pipeline dag: tasks: - name: build template: kaniko - name: package dependencies: [build] template: helm-package - name: push-chart dependencies: [package] template: helm-push - name: approve-dev dependencies: [push-chart] template: pause - name: sync-dev dependencies: [approve-dev] template: argocd-sync arguments: parameters: [{name: app, value: nginx-dev}] - name: approve-prod dependencies: [sync-dev] template: pause - name: sync-prod dependencies: [approve-prod] template: argocd-sync arguments: parameters: [{name: app, value: nginx-prod}] # -- Kaniko 构建并推镜像 -- - name: kaniko outputs: parameters: - name: tag valueFrom: path: /tmp/tag container: image: gcr.io/kaniko-project/executor:v1.20.0 env: - name: TAG value: \u0026#34;{{workflow.labels.git-sha}}\u0026#34; args: - --dockerfile=Dockerfile - --destination=harbor.example.com/library/nginx:${TAG} lifecycle: postStart: exec: command: [\u0026#34;sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;echo ${TAG} \u0026gt;/tmp/tag\u0026#34;] volumeMounts: - name: harbor-auth mountPath: /kaniko/.docker volumes: - name: harbor-auth secret: secretName: harbor-secret # -- Helm Package（写 Dev/Prod） -- - name: helm-package inputs: parameters: - name: tag value: \u0026#34;{{tasks.build.outputs.parameters.tag}}\u0026#34; script: image: alpine/helm:3.13.0 command: [sh] source: | TAG=$0 cp -R /workspace/chart /tmp/chart \u0026amp;\u0026amp; cd /tmp/chart for env in dev prod; do yq -i \u0026#39;.image.tag = strenv(TAG)\u0026#39; values-${env}.yaml done yq -i \u0026#39;.version=strenv(TAG) | .appVersion=strenv(TAG)\u0026#39; Chart.yaml helm dependency update . helm package . -d /tmp/out echo /tmp/out/$(ls /tmp/out) \u0026gt;/tmp/chart env: - name: 0 value: \u0026#34;{{inputs.parameters.tag}}\u0026#34; outputs: artifacts: - name: chart path: /tmp/chart # -- Helm Push 到 Harbor OCI -- - name: helm-push inputs: artifacts: - name: chart from: \u0026#34;{{tasks.helm-package.outputs.artifacts.chart}}\u0026#34; script: image: alpine/helm:3.13.0 command: [sh] source: | helm registry login harbor.example.com -u $USER -p $PASS helm push $(cat {{inputs.artifacts.chart}}) oci://harbor.example.com/chartrepo env: - name: USER value: robot$library+ci - name: PASS valueFrom: secretKeyRef: name: harbor-secret key: .dockerconfigjson - name: pause suspend: {} - name: argocd-sync inputs: parameters: - name: app container: image: argoproj/argocd:v2.10.0 command: - argocd - app - sync - \u0026#34;{{inputs.parameters.app}}\u0026#34; - --grpc-web - --insecure env: - name: ARGOCD_AUTH_TOKEN valueFrom: secretKeyRef: name: argocd-token key: token 5️⃣ Argo CD Application + Rollout（金丝雀） # values.yaml 里使用 Rollout，示例略（见前文 Rollouts 示例）。\n将 autoPromotionEnabled: false 可以让运维手动切流量。\n🔑 关键点 # 步骤 技术 备注 事件捕获 Argo Events Git Webhook CI/Image Argo Workflows + Kaniko 无 docker-in-docker 审批 Workflow Suspend UI / CLI Resume GitOps Workflow Git commit 改 chart tag CD \u0026amp; Canary Argo CD + Rollouts Dev / Prod 集群 指标分析 Prometheus Rollouts Analysis 按需扩展更多服务可用 ApplicationSet Matrix。\n如需完整示例仓库，请将上面 YAML 放入新版 GitOps 存储结构。\n"},{"id":90,"href":"/docs/%E7%AE%97%E6%B3%95/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/","title":"拓扑排序","section":"算法","content":"本章节介绍堆\n基本思想 # 拓扑排序（Topological Sorting）是 针对有向无环图（DAG, Directed Acyclic Graph）的一种排序方法，它的目标是将图中的所有顶点排序，使得对于每一条有向边 (u → v)，顶点 u 先出现在 v 之前。\nkahn算法（入度法） 维护所有入度0的节点（可以入队） 每次从队列中取出一个节点，并删除它指向的边，更新相邻节点的入度 若某个相邻节点入度变成0，则加入队列 知道所有节点都被访问完 DFS 逆序法 采用DFS遍历每个点，递归访问所有子节点 访问完所有后续节点后，将当前的节点压入栈 最终栈中的元素顺序即为拓扑排序的结果 经典应用 # 任务调度 场景：多个任务有前后依赖关系，必须按顺序执行。 示例：编译多个源代码文件时，需要先编译依赖文件。 课程安排（先修课程） 场景：某些课程需要先完成前置课程，才能学习后续课程。 示例：LeetCode 207 课程表（Course Schedule）问题。 电路时序分析 场景：计算电路中信号的传播顺序，确保数据按逻辑流动。 软件包依赖管理 场景：如 npm、pip 等包管理器，安装包时需要先安装其依赖项。 编译器优化 场景：代码中的函数调用优化，确定依赖关系，减少重复计算。 优缺点 # 优点\n高效性： Kahn 算法 时间复杂度 O(V + E)（遍历所有节点和边）。 DFS 逆序法 时间复杂度也是 O(V + E)。 易于实现： 代码简单，适用于有向无环图（DAG）。 解决依赖关系问题： 能很好地解决 任务调度、先修课程、软件包管理 等依赖问题。 缺点\n仅适用于 DAG： 不能处理有环图，如果图中有环，无法进行拓扑排序。 可能有多种拓扑排序结果： 拓扑排序的结果可能不是唯一的，不同的算法可能得出不同的合法顺序。 不能检测所有依赖冲突： 仅能发现 循环依赖（环），但无法处理复杂的冲突（如并发依赖）。 "},{"id":91,"href":"/docs/%E7%AE%97%E6%B3%95/%E5%88%86%E6%B2%BB%E6%B3%95/","title":"分治法","section":"算法","content":"本章节介绍贪心算法\n基本思想 # 分解 将多个问题分成多个规模较小、相互独立的子问题 这些子问题的形式与原问题相似，但规模较小 解决 递归地求解这些子问题，直到问题的规模足够小，可以直接求解（通常较小规模的基本情况，如数组只有一个元素） 合并 将所有的子问题的解合并成原问题的解 经典应用 # 排序算法 归并排序（Merge Sort）：将数组拆分成两半，分别排序后合并。 快速排序（Quick Sort）：选择一个基准值（pivot），将数组划分为 小于基准值 和 大于基准值 的部分，然后递归排序。 计算数学 大整数乘法（Karatsuba算法）：将大整数拆分为较小的部分递归计算，提高乘法速度。 矩阵乘法（Strassen算法）：将矩阵拆分成更小的子矩阵，提高计算效率。 递归问题 汉诺塔问题：将问题分解为“移动 n-1 层盘子”的问题递归求解。 最大子数组和（分治法解法）：将数组拆分为左右两部分，求解最大子数组和。 计算几何 最近点对问题：将点集按 x 轴拆分，分别计算最近点对，然后合并结果。 凸包问题：使用分治法求解点集中凸包。 优缺点 # 优点\n易于理解和实现：递归思想清晰，代码结构往往简单直观。 高效：通常可以将问题的时间复杂度降至 O(n \\log n) 或更优，如 归并排序、快速排序。 适用于大规模问题：能够将复杂问题拆解成小问题，适用于 大数据处理。 并行计算：许多分治法算法可以 并行化（如 并行归并排序），提升执行效率。 缺点\n递归调用可能造成额外开销：如果递归深度过大，可能导致 栈溢出（Stack Overflow）。 额外的空间开销： 归并排序需要 额外的存储空间（O(n)）来存储合并后的结果。 快速排序（在原数组上进行，不需要额外空间）。 子问题可能不是完全独立的： 例如，动态规划 也使用递归，但存在 重叠子问题，需要通过 记忆化搜索（Memoization） 或 递推 进行优化。 "},{"id":92,"href":"/docs/%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"二叉树","section":"算法","content":" 本章节介绍二叉树 # 二叉树的性质 # 节点总数： 深度为 h 的满二叉树最多有 2^{h} - 1 个节点。 高度（height） 从当前节点到叶子节点的最长路径上的边数 根节点的高度=整棵树的高度 深度（depth） 根节点的深度为0，某个节点的深度是从根节点到该节点的路径长度（边的数量） 层（level） 根节点为第1层，其子节点为第2层，以此类推 特殊类型的二叉树 # 满二叉树 # 每个节点要么有0个节点，要么有2个子节点（不存在只有一个节点的情况） 叶子节点都在同一层或者仅比最底层少一层 完全二叉树 # 除了最后一层，其他层的节点必须是满的 最后一层的叶子节点必须靠左排列 完美二叉树 # 所有的叶子节点必须处于同一层 节点数满足公式 2^h - 1（h 为高度） 平衡二叉树 # 任意节点的左右子树高度差不超过 1 典型代表：AVL 树，红黑树 二叉搜索树 # 左子树的所有节点值 \u0026lt; 根节点 \u0026lt; 右子树的所有节点值 中序遍历的结果是一个递增序列 二叉堆 # 最小堆：父节点 \u0026lt;= 子节点 最大堆: 父节点 \u0026gt;= 子节点 二叉树的遍历方式 # 1 / \\ 2 3 / \\ / \\ 4 5 6 7 / \\ / \\ / \\ / \\ 8 9 10 11 12 13 14 15 前序遍历 # 根 → 左子树 → 右子树\n访问当前节点 递归遍历左子树 递归遍历右子树 [1, 2, 4, 8, 9, 5, 10, 11, 3, 6, 12, 13, 7, 14, 15] 实现方式：递归/栈\n适用于：拷贝二叉树、前缀表达式计算。\n中序遍历 # 左子树 → 根 → 右子树\n递归遍历左子树 访问当前节点 递归遍历右子树 [8, 4, 9, 2, 10, 5, 11, 1, 12, 6, 13, 3, 14, 7, 15] 实现方式：递归/栈\n适用于：二叉搜索树（BST） 的升序遍历\n后续遍历 # 左子树 → 右子树 → 根\n递归遍历左子树 递归遍历右子树 访问根节点 [8, 9, 4, 10, 11, 5, 2, 12, 13, 6, 14, 15, 7, 3, 1] 实现方式：递归/两个栈\n适用于：删除节点、计算树的高度。\n层序遍历 # 依次访问每一层的节点，从上到下，从左到右 适用于：最短路径搜索（如 BFS 算法）\n"},{"id":93,"href":"/docs/go/sync/sync_pool_guide/","title":"Sync Pool Guide","section":"sync","content":" Go sync.Pool 深度指南 # 适用版本：Go 1.22（机制自 1.3 引入，1.13 加入 Victim 缓存）\n定位：高频、短生命周期对象的临时复用池，减轻 GC 压力。\n1 · 内部机制概览 # 组件 作用 Per‑P freelist 每个逻辑处理器 P 拥有本地空闲链，命中时完全无锁 Victim list 每轮 GC 把 freelist 交接到 victim；两轮无命中则让 GC 回收 New 回调 Get 失败时调用，仅当次 返回，不会自动入池 GC 清空 Pool 对象无引用即随 GC 清空，不做长期缓存保证 流程：Put → freelist → (GC) → freelist ↔ victim → 两轮无用 → GC 回收。\n2 · API 快览 # type Pool struct { New func() any } p := sync.Pool{New: func() any { return new(bytes.Buffer) }} obj := p.Get() // 可能是 nil /* ...使用... */ p.Put(obj) // 归还，可放 nil 3 · 使用示例 # 3.1 复用 bytes.Buffer（Web JSON 响应） # var bufPool = sync.Pool{ New: func() any { return new(bytes.Buffer) }, } func writeJSON(w http.ResponseWriter, v any) { buf := bufPool.Get().(*bytes.Buffer) buf.Reset() defer bufPool.Put(buf) _ = json.NewEncoder(buf).Encode(v) w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w.Write(buf.Bytes()) } 收益：在 50 k QPS 测试中，allocs/op 从 4.2 → 0.3，P99 延迟 ‑15 %。\n3.2 重复利用大 Slice（压缩工具） # type scratch struct{ buf []byte } var scratchPool = sync.Pool{ New: func() any { return \u0026amp;scratch{buf: make([]byte, 0, 64\u0026lt;\u0026lt;10)} }, // 64 KiB } func Compress(dst io.Writer, src []byte) error { sc := scratchPool.Get().(*scratch) defer scratchPool.Put(sc) sc.buf = sc.buf[:0] out := snappy.Encode(sc.buf, src) _, err := dst.Write(out) return err } 4 · 基准测试对比 # Benchmark ns/op B/op allocs/op make([]byte) 12 500 65 536 1 sync.Pool 320 0 0 Pool 把分配时间降低到 ~3 %，并做到 零分配。\n5 · 最佳实践 # Put 前 Reset：清理可变字段，防止脏数据泄露。 存指针而非值：减少 interface 复制成本。 Benchmark 决策：用 go test -benchmem、pprof 量化收益。 避免放稀缺资源：FD、DB 连接需显式生命周期，Pool 随 GC 可能丢失。 低 QPS 无意义：额外原子开销可能得不偿失。 6 · 常见误区 # 误区 事实 “Pool 会无限增长” 每轮 GC 最多保留两代，冷对象最终被回收 “必须 Put 零值” 只需 可安全复用；保留底层缓冲才有意义 “Pool 相当于缓存” 不保证存活时间，也不支持淘汰策略 7 · 何时 不要 用 sync.Pool # 小对象、低并发，GC 开销可忽略。 需要 TTL/LRU 逻辑。 阻塞 I/O，占用线程时间 \u0026raquo; 分配时间。 8 · 版本演进 # 版本 变更 效益 1.3 初版发布 支持 GC 清空 1.13 Victim list 减少 GC 突刺 1.21 fast‑path 优化 高并发吞吐 ↑ 一句话总结 # sync.Pool 让“短命大对象”在下一次被用到前有个落脚点：降低 GC，简化回收，不做长期缓存。\n"},{"id":94,"href":"/docs/go/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/pprof_analysis/","title":"Pprof Analysis","section":"性能测试","content":" Go pprof 性能分析实践手册 # 适用版本：Go 1.20 – Go 1.23（2025 年最新版）\n阅读对象：希望系统掌握 pprof 并落地到生产的 Go 开发 / SRE\n目录 # 概览：pprof 能解决什么问题 Profile 类型与采样原理 离线采集：基准测试 \u0026amp; CLI 在线采集：net/http/pprof \u0026amp; 远程抓取 可视化：命令行、FlameGraph、Speedscope 案例一：CPU Hotspot 定位与优化 案例二：内存泄漏排查 案例三：阻塞 / Mutex 竞争分析 持续 Profiling：Parca / Pyroscope / Grafana Pyroscope Cloud PGO：Profile-Guided Optimization 流程 自动化工作流与 CI 集成 常见误区 \u0026amp; 调参清单 参考链接 1. 概览：pprof 能解决什么问题 # 定位 CPU 热点：采样型 Profile，找到最耗时的函数或调用链 排查内存泄漏：对比多份堆快照，确定分配点 锁竞争 / 阻塞：查看被 sync.Mutex 或系统调用阻塞的时间 Goroutine 爆炸：快照当前所有 goroutine 栈，分析创建源 pprof 不会直接告诉你“怎么改”，但能精准指向瓶颈位置。随后通过代码 / 参数 / 架构层面优化并 benchstat 验证。\n2. Profile 类型与采样原理 # Profile 默认采样 主要字段 CPU 100 Hz 时钟中断 flat, cum **Heap (allocs/inuse)` 每分配 512 KiB alloc_objects / inuse_space Block 100 Hz contentions_ns Mutex 50 Hz contentions, delay Goroutine 快照 stack Go 运行时在热点处插入采样回调，成本≈1–2 % CPU。 Heap 采样率可通过环境变量 GODEBUG=memprofilerate=N 调节。 3. 离线采集：基准测试 \u0026amp; CLI # # 编译基准并输出 cpu.prof go test -run=^$ -bench=. -benchtime=10s -cpuprofile cpu.prof ./pkg # 生成内存 profile GODEBUG=memprofilerate=131072 go test -run=^$ -bench=. -memprofile mem.prof ./pkg Tips\n-run=^$ 跳过单元测试，仅跑基准 -benchmem 报告 alloc/op，配合 mem.prof 效果更佳 4. 在线采集：net/http/pprof # import _ \u0026#34;net/http/pprof\u0026#34; func main() { go http.ListenAndServe(\u0026#34;:6060\u0026#34;, nil) ... } 常用抓取命令 # URL 说明 /debug/pprof/profile?seconds=30 CPU 30 s /debug/pprof/heap 堆快照 /debug/pprof/mutex?debug=1 Mutex /debug/pprof/block?debug=1 Block 确保只对内网或 mTLS 暴露； kubernetes 可用 kubectl port-forward 临时拉通。\n5. 可视化 # 5.1 CLI # go tool pprof -top cpu.prof go tool pprof -list \u0026#39;MyHotFunc\u0026#39; cpu.prof 5.2 内置 Web # go tool pprof -http=:8080 cpu.prof 5.3 Speedscope # go tool pprof -proto cpu.prof \u0026gt; cpu.pb.gz # 将 protobuf 文件拖到 https://www.speedscope.app Speedscope 支持时间轴视图，适合分析并行 / I/O 阻塞。\n6. 案例一：CPU Hotspot # 症状：QPS 达到 5k 时 CPU 占用 900 % 采集：30 秒 CPU profile 分析：top 发现 regexp.(*Regexp).FindAllIndex flat=45 % 优化： 复用编译后的正则 替换为 strings.Contains + 预解析 回归：benchstat 显示 latency ↓30 % / CPU ↓25 % 7. 案例二：内存泄漏 # # 周期抓取堆 go tool pprof http://app:6060/debug/pprof/heap -o heap1.prof sleep 120 go tool pprof http://app:6060/debug/pprof/heap -o heap2.prof # Diff go tool pprof -base heap1.prof heap2.prof 若差异集中在 bytes.makeSlice，通常是切片无限扩容；检查 cap 预估或扩容策略。\n8. 案例三：阻塞 / Mutex # Block profile：定位网络 / DB 延迟 Mutex profile：出现 runtime.mapassign 热点 → map 写争用；可换 sync.Map 或 sharding Go 1.21+ 支持 runtime/pprof.Labels(\u0026quot;tenant\u0026quot;, id)，在火焰图中按标签聚合，快速识别哪条租户耗时。\n9. 持续 Profiling # 方案 特点 Parca / Parca Agent 云原生，eBPF + DWARF，无需 sidecar Pyroscope UI 友好，可与 Grafana Merge Go 运行时 mprof 1.20+ 内置连续 CPU/Heap 采样，低开销 在 Kubernetes 集群推荐 DaemonSet 方式安装 Agent，统一上报到集中 UI。\n10. PGO：Profile-Guided Optimization # Go 1.20 起支持 -pgo= 流程： go test -c -pgo=./pgo.pprof go build -pgo=./pgo.pprof -o app PGO 主要优化 内联 / 分支预测；对微服务效果 3–10 %。 11. 自动化工作流 # # Makefile 片段 bench: go test -run=^$ -bench=. -benchmem -cpuprofile cpu.prof ./... benchstat old.txt new.txt go tool pprof -svg cpu.prof \u0026gt; cpu.svg CI 中上传 cpu.svg 作为 artifact，配合 GitHub Actions 注释 PR。\n12. 常见误区 \u0026amp; 调参清单 # 误区 纠正 抓一次 profile 就够 采样有随机性，至少多跑 3 次 Heap Profile = RSS 只包含 Go 堆，不含 mmap、Cgo、栈 pprof 会拖慢线上 采样型开销极低，阻塞 / mutex 建议临时开启 调参\nruntime.SetMutexProfileFraction(10) – 每 10 次竞争采样一次 runtime.SetBlockProfileRate(1) – 捕捉所有阻塞（排障后改回 0） 13. 参考链接 # Go 官方 Blog《Profiling Go Programs》 Brendan Gregg FlameGraph 项目 Parca.dev / Pyroscope.io / Grafana Cloud Continuous Profiling Go 1.23 Release Notes – PGO \u0026amp; profiler 改进 建议：在每个重大版本升级后，重新评估采样率与阈值。Go 运行时及 GC 每年都在进步，历史“玄学调参”可能已过时。\n"},{"id":95,"href":"/docs/go/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/go_escape_analysis/","title":"Go Escape Analysis","section":"数据结构","content":" Go 逃逸分析（Escape Analysis）详解 # 核心结论：逃逸分析发生在 编译期，用于判断一个变量在函数调用链上是否可能“逃出”其栈帧——一旦逃逸，就必须分配到堆上并受垃圾回收管理；否则可在栈上分配，生命周期随栈帧自动结束。\n1 · 为什么需要逃逸分析？ # 目标 解释 减少 GC 压力 栈上分配无需回收；堆分配越少，GC 次数和停顿越低。 加速访问 栈内存比堆内存局部性好，CPU Cache 友好。 消除同步 栈私有，无需并发同步；堆对象可能被其他 goroutine 间接引用。 2 · Go 编译器的逃逸判定逻辑 # 变量生命周期分析 建立 SSA（静态单赋值）图，跟踪变量从定义到所有可能的使用点。 指针追踪 若变量地址被传递到： 返回值 / 指针参数 接口值（装箱） 堆对象字段 反射、unsafe.Pointer、闭包捕获\n则视为“可能在当前调用栈外仍可访问” → 堆分配 循环迭代合并 (Promoted Scalars) 对仅在循环内、且不被取地址的临时对象，可做 Scalar Replacement of Aggregate (SRA)，彻底拆成寄存器/栈标量。 跨函数内联协同 //go:noinline 或过大函数体会阻断内联，导致更多逃逸。 反之，内联后遗漏取地址操作可被进一步分析为栈分配。 原则：编译器宁可“误杀”也不会“漏判”。一旦逃逸可能，直接分配堆。\n3 · 查看逃逸信息 # go build -gcflags=\u0026#34;-m -l\u0026#34; # -m 打印逃逸信息，-l 禁止内联便于观察 示例代码 escape.go：\npackage main type Data struct{ buf [1024]byte } func stackAlloc() *Data { d := \u0026amp;Data{} // \u0026amp;Data 可能逃逸 return d // 返回出函数 → 堆 } func stayInStack() int { x := 42 // 不取地址 return x // 栈上 } func main() { _ = stackAlloc() _ = stayInStack() } 编译输出（截取）：\n./escape.go:6:6: new(Data) escapes to heap // d = \u0026amp;Data{} ./escape.go:15:6: moved to heap: d // main 调用结果仍在堆 阅读技巧\n“escapes to heap” → 变量本体在堆，指针变量仍在栈；\n“moved to heap” → 编译器将整个变量搬到堆上。\n4 · 常见导致逃逸的场景 # 场景 示例 说明 / 规避策略 返回局部指针 return \u0026amp;v 必逃逸；可改为值返回或接口回收池。 闭包捕获 go func() { println(v) }() v 生命周期 \u0026gt; 外层栈帧；考虑传值。 接口装箱 var i interface{} = struct{...}{} 非空接口把数据复制进去；空接口将指针+类型逃逸。 fmt/反射 fmt.Println(v) fmt 参数为接口，编译期不可见；临时值常逃逸。 slice 追加 append(s, bigStruct) 若 slice 超容量并返回到上层，则底层数组可能逃逸。 method 取址 obj.Method() 若 Method 需要接收者地址 使用值接收者版本 func (T) Method()。 5 · 优化实践 # 多用值语义：小结构体 (\u0026lt; 128 B) 尽量值传递；指针多意味着逃逸风险。 复用缓冲区：sync.Pool 缓解频繁堆分配，但别过度（GC 还是会清）。 合理切分函数：过大函数不易内联；拆分后热点路径可更精准栈分配。 避免不必要的接口 \u0026amp; 反射：热点内层用泛型或直接类型。 profile 验证：go test -bench . -benchmem / pprof -alloc_space；确保修改确实降低 B/op（每次操作分配字节数）。 6 · 与 Go 版本演进 # 版本 改进 影响 Go 1.13 新 inlining + escape 框架 减少闭包捕获逃逸 Go 1.20 整型范围分析 合并到 EA 对 map key 临时变量更友好 Go 1.22 SRA \u0026amp; φ‑节点优化 slice/header 拆分更彻底 Go 1.23（规划） Path-sensitive EA 复杂条件分支下更精准 7 · 微基准例子（观察逃逸前后） # package bench type big struct{ buf [4096]byte } func byPointer() *big { b := \u0026amp;big{} // 堆 b.buf[0] = 1 return b } func byValue() big { var b big // 栈（4 KiB 会被 memclr，但仍在栈） b.buf[0] = 1 return b // 复制开销，但可由编译器变成 `memmove` } go test -bench . -benchmem 输出示例（不同机器略有差异）：\nBenchmarkByPointer-8 1000000 1200 ns/op 4096 B/op 1 allocs/op BenchmarkByValue-8 1000000 320 ns/op 0 B/op 0 allocs/op 8 · 调试陷阱 # //go:escape 内置注释（runtime 包使用）强制变量视为逃逸；普通项目请勿使用。 unsafe 误导 编译器对 unsafe.Pointer 会 保守逃逸，除非能证明只在栈闭包内访问。 cgo 传给 C 的指针禁止是 Go 栈指针（CGO Check）；因此几乎都要堆分配。 9 · 一图速查 # ┌───────────┐ \u0026amp;v / interface / closure │ 变量定义 │───────可能逃逸─────────┐ └───────────┘ │ │SSA 栈分析 ▼ │ ┌───────────┐ └────────No───────────▶│ 栈上分配 │ │ └───────────┘ │Yes ▼ ┌──────────────┐ │ 堆上分配 + GC │ └──────────────┘ 小结 # 逃逸分析是 Go 性能优化的第一道关卡：堆 vs. 栈 决策全凭编译期推导。 使用 -gcflags=-m 审视热点函数的逃逸输出，配合 benchmem \u0026amp; pprof 做量化回归。 不必“谈逃逸色变”——在可读性和性能之间找到平衡，代码的清晰度 往往比极端的栈优化更重要。 "},{"id":96,"href":"/docs/go/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/go_gc_overview/","title":"Go Gc Overview","section":"数据结构","content":" Go 垃圾回收（GC）工作原理全景 # 适用版本：Go 1.22；从 1.5 并发三色标记‐清扫奠基，后续演进始终保持总体框架不变。\n关键词：并发三色、写屏障、Mutator Assist、Scavenger、STW 压缩、Soft Memory Limit。\n1 · 总体架构 # 维度 特性 说明 算法 并发三色 Mark‑Sweep 绝大部分标记、清扫与业务线程并发执行 停顿 两次 STW Mark Setup \u0026amp; Mark Termination，通常 \u0026lt; 1 ms 并发 后台/辅助标记 + 清扫 业务线程按“分配比例”主动干 GC，后台 goroutine 持续工作 内存回收 Span 清扫 + Scavenger 小对象归还到 per‑P 空闲链，大 Span 退还操作系统 2 · 一次 GC 循环 # ┌──────────────┐ ┌─────────────────────────────┐ │ STW #1 │ │ STW #2 │ │ Mark Setup │──▶──▶│ Mark Termination (黑完灰) │ └─▲────────────┘ └────────▲────────────────────┘ │ │ │并发写屏障 │并发 Sweep / Scavenge └───► 并发标记 (Mutator Assist + BG marker) ────►┘ 2.1 阶段要点 # 阶段 作用 典型耗时 (8C/2 GiB) Mark Setup (STW) 根对象灰化（栈/全局/寄存器）；打开写屏障 30–200 µs 并发标记 灰 → 黑；Mutator Assist 按分配量帮忙 1–100 ms (并发) Mark Termination (STW) 灰队列清空；停写屏障；记录 liveBytes 50–400 µs 并发清扫 (Sweep) 遍历 span，将空闲块归还 mcentral 1–100 ms (并发) Scavenger 大块空闲页 madvise 回 OS，压 RSS 持续后台 3 · 三色标记 \u0026amp; 写屏障 # 3.1 颜色含义 # 颜色 状态 行为 白 未访问 可能垃圾 灰 已发现未扫描 等待扫描 黑 已扫描 活对象 不变式：黑对象不能指向白对象。\n写屏障负责在 Mutator 并发写指针时，把 旧值 或 新值 灰化，维持不变式。\n3.2 Hybrid Write Barrier # dstOld := *p // 读取旧指针 * p = newPtr // 写新指针 shade(dstOld) // 灰化旧值 编译器在插桩阶段对 “可能在 GC 时写指针” 的指令包裹 writebarrierptr。 只有在 并发标记窗口 内屏障才做实事，其余时间是空函数 → 99.9% 路径零开销。 4 · Mutator Assist 机制 # 分配路径会累加 assist debt\ndebt += bytesAllocated * (markWorkRate / allocRate) 若负债超阈值，则在分配调用栈中 主动扫描灰对象，直到偿还债务。 这样实现 “GC 速度 ≥ 分配速度” 的自适应节奏，无需频繁全局锁。 5 · Heap 布局与清扫 # 概念 粒度 功能 Span 8 KiB–4 MiB 内存管理基本单元，记录对象位图 Class (size class) 8 B–32 KiB 共 67 档 小对象按 class 分配，复用缓存 MCache (每 P) 线程/处理器本地空闲链 分配无需锁 MCentral 全局空闲链 Sweep 将 Free span 回收 Sweep：\n读取 span 位图，黑对象所在 block 标记保留。 其余 block 归并到 freeList；若整 Span 空，挂到 mheap.sweepSpansEmpty。 分配若无缓存，会触发 On‑Demand Sweep。 6 · Scavenger (退还物理内存) # Go 1.17 起 背景线程 + Bitmap 追踪 unused 页。 达阈值后调用 madvise(MADV_DONTNEED) → OS 立刻回收物理页，RSS 下降。 受 GODEBUG=madvdontneed=1 控制；默认为开。 7 · 堆目标 \u0026amp; 节奏控制 # targetHeap = liveHeap * (1 + GOGC/100) // 默认 GOGC=100 分配达到 targetHeap → 触发下一轮 GC。 GOGC↓ 提前触发、降低峰值内存；GOGC↑ 反之但暂停次数减。 Go 1.19 引入 Soft Memory Limit：若进程 RSS 超阈值，GC 会加倍速率直至降回。 8 · 观测指标速查 # Metric (Prometheus/runtime) 解读 gc.pause_ns.sum / gc.pause_total_ns 累计 STW；尾延迟敏感业务建议 \u0026lt; 100 ms/min gc.cycles.total / gc.heap_objects GC 频率 \u0026amp; 存活对象趋势 gc.cpu.ns / process_cpu_seconds_total GC CPU 比例；\u0026gt;30% 需优化 heap_released vs heap_idle 释放给 OS 的页 vs 未用页；碎片观测 gc.assist_cpu_fraction (1.22) Mutator Assist 占用；过高说明分配过猛 9 · 版本演进亮点 # 版本 重要变化 效果 1.14 Async preempt；函数中途抢占 STW 压缩 × 2 1.17 新 Scavenger；Huge Page 感知 容器 RSS ‑30% 1.19 Soft Memory Limit；Pacing 重写 OOM 风险显著下降 1.22 写屏障路径简化；灰队列优化 2 GiB 堆 STW \u0026lt; 1 ms 1.23 (计划) 年轻代（Generational）原型 进一步削减短命对象成本 10 · 实战建议 # 先量化：打开 runtime/metrics → Prometheus，确定究竟是暂停、CPU、还是 RSS 瓶颈。 热点代码审逃逸：go build -gcflags=-m + benchmem 量化 allocs/op。 合理调 GOGC：内存紧 → 50–80；CPU 忙 → 150–200；上线前压测找平衡点。 容器环境：设置 GOMEMLIMIT 或 GODEBUG=softmemlimit=…，避免 K8s OOMKill。 升级 Go 版本：大版本 GC 改进显著，优先考虑 1.22+。 一句话记忆 # “极短 STW + 并发标记 + 自助助攻 + 背景清扫 + 物理页回收” = Go GC 成熟方案。\n若你手上有 gctrace 日志、heap pprof、trace.out 等文件，或想针对特定场景（延迟、内存、CPU）调优，随时发给我，我们可以逐行解读并给出参数与代码级建议。\n"},{"id":97,"href":"/docs/go/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/go_gmp_model/","title":"Go Gmp Model","section":"数据结构","content":" Go 运行时调度：G‑P‑M 模型全解 # 适用版本：Go 1.22（设计自 1.2 奠基，1.14 搭配异步抢占形成当前形态）\n核心概念：Goroutine (G)、Processor (P)、Machine (M)\n关键词：Work‑stealing、Run Queue、Syscall Block、Netpoll、Async Preempt\n1 · 为什么要有 G‑P‑M？ # 目标 问题 解决方案 高并发 Goroutine 数可达百万；稀缺资源是核数 引入 P 承载调度上下文 低成本切换 OS 线程切换昂贵（\u0026gt;1 µs） 在用户态切换 G (~30 ns) 充分利用多核 全局锁 \u0026amp; 线程竞争严重 Work‑stealing + 局部队列 隐藏阻塞 系统调用/网络 IO 可能卡核 M \u0026lt;=\u0026gt; P 解绑，阻塞不拖慢调度 2 · 三大角色 # 2.1 G — Goroutine 控制块 # type g struct { stack stack // [lo, hi) sched gobuf // 保存寄存器快照 status uint32 // _Grunnable, _Grunning, ... m *m // 正在执行该 g 的 M stackguard0, stackguard1 uintptr // 抢占 \u0026amp; 缺栈检查 // ...省略 } 生命周期：_Gidle → _Grunnable → _Grunning → _Gwaiting/_Gdead … 特殊 g：g0 (系统栈)、gsignal (信号栈) 2.2 P — Processor (逻辑 CPU) # 字段 含义 runq [256]g 每 P 本地环形队列 runqhead/runqtail 无锁环队列指针 schedtick 调度计数；抢占门控 mcache 本地对象缓存 (alloc fast‑path) gcAssistTime GC 助攻计时 P 的数量 = GOMAXPROCS，决定并发度上限。\n2.3 M — Machine (绑定 OS 线程) # 字段 说明 g0 系统栈；执行调度、GC curg 当前运行的 G p 绑定的 P (可空) park 当无 P 可用时睡眠的 semaphore M 可 多于 P（阻塞系统调用需额外线程）。 3 · 调度循环核心流程 # ┌─ G 进入 runnable ─┐ │ newproc/go func │ └───────┬───────────┘ ▼ P.runq ← push ▼ no local G ? ┌── execute G ──┐ ──► steal from other P ─┐ │ switch to g │ │ └───► run g │ ▼ │ global runq ? │ syscall/block ───────────────► steal from global ▼ hand P back to sched 3.1 本地优先 \u0026amp; Work‑stealing # M 从 P.runq Pop G，若空： 从全局队列 sched.runq 抢 0.5 批量 随机挑一半 P 尝试 steal 1/2 G 新 G 创建时优先入 当前 P.runq；若满再入全局。 3.2 系统调用处理 # 进入：g 进入 _Gsyscall；M 放回 P，自身进入 syscall 队列。 返回：唤醒休眠 M 或新建 M 以接管空闲 P。 3.3 网络 Poller \u0026amp; Timer # netpoll 轮询 epoll/kqueue；就绪 G 直接放入当前 P 或全局队列。 timer 到期后同样入队。 3.4 抢占 # 方式 版本 触发 协作抢占 全版本 函数序言 stackguard0 检查 异步抢占 ≥1.14 向线程发送信号，中断到 g0 4 · 状态机一览 # +---------+ | _Gidle | +----+----+ | new G | acquire P v +---------+ runnable +-----------+ | _Gdead |-----------\u0026gt; | _Grunnable| +---------+ +-----+-----+ | schedule v +-----+-----+ | _Grunning | +--+---+----+ | | syscall/block| |time slice | | +------+------+----+ | _Gsyscall | _Gwaiting +------------+-------+ 5 · 关键源码导航 # 文件 关注函数 / 结构 说明 runtime/proc.go schedule, execute, newproc, goexit 调度主循环与 G 创建、销毁 runtime/runtime2.go type g, type m, type p 三大核心数据结构 runtime/asm_*.s goschedguarded, asminit 汇编入口、寄存器保存 runtime/netpoll_kqueue.go / _epoll.go netpoll I/O 多路复用集成 6 · 调优与观测 # 场景 指标 / 工具 建议 调度瓶颈 go tool trace → “Scheduler Latency” 观察 Proc Idle vs Runnable 过量线程 M-\u0026gt;P Ratio (runtime metrics) 频繁 syscall → 优化 I/O 模型 栈拷贝高 pprof gcsamples-bytes 避免深递归 \u0026amp; 大局部变量 抢占延迟 trace GoSched/GoPreempt runtime/debug.SetMaxThreads(…) 7 · 常见问题排查 # 症状 可能原因 排查 CPU 利用率低 GOMAXPROCS \u0026lt; CPU 或 goroutine 阻塞多 runtime.GOMAXPROCS(0) / trace 高延迟尖刺 系统调用阻塞；抢占不及时 sysmon 日志 / schedtrace=1000 线程爆炸 cgo 或反复陷入阻塞 ps -L / debug.SetMaxThreads 8 · 版本演进亮点 # 版本 改进 影响 1.2 引入 P、本地 runq 大幅提升并发度 1.10 netpoll 升级 \u0026amp; timer 分层 定时器效率↑ 1.14 Async Preempt 抢占延迟从 ms → µs 1.20 调度器 trace 重写 可视化更友好 1.22 Goroutine Aware Alloc (mcache) 分配快‑path 进一步本地化 9 · 实战 Tips # 优先本地化：避免在热点循环中创建/销毁 goroutine，可用对象池或 channel 缓冲复用。 合理 GOMAXPROCS：I/O 型服务可略小于 CPU 核；计算型贴合物理核数。 善用 go tool trace：调度可视化最直接。 避免长阻塞：数据库/网络调用加入超时；必要时用 semaphore 控并发。 使用 GODEBUG=schedtrace=1000,scheddetail=1 观测实时 Runq 长度。 一句话总结 # M 执行 G，P 提供执行权，runq + steal 让 goroutine 在多核间均衡，高并发低开销。\n如需对 trace 火焰图、schedtrace 日志 做深入解析，或定位调度延迟尖刺，欢迎将数据发给我，一起逐帧分析。\n"},{"id":98,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-event/example/","title":"Example","section":"argo event","content":" 🚀 Argo Events 支持的 Trigger 类型 + Sensor + EventBus 示例合集 # ✅ EventBus（NATS 默认）示例 # apiVersion: argoproj.io/v1alpha1 kind: EventBus metadata: name: default namespace: argo-events spec: nats: native: replicas: 3 auth: token ✅ Sensor + Trigger 示例合集 # 📌 1. Kubernetes Trigger - 创建 Workflow # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: workflow-sensor namespace: argo-events spec: dependencies: - name: webhook-dep eventSourceName: webhook-source eventName: webhook triggers: - template: name: create-workflow k8s: group: argoproj.io version: v1alpha1 resource: workflows operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: example-workflow- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026#34;Hello from Argo Events\u0026#34;] 📌 2. HTTP Trigger - 调用外部 Webhook # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: http-trigger-sensor spec: dependencies: - name: webhook-dep eventSourceName: webhook-source eventName: webhook triggers: - template: name: call-http http: url: https://hooks.example.com/build method: POST payload: - src: dependencyName: webhook-dep dataKey: body dest: body.data headers: - name: Content-Type value: application/json 📌 3. Kafka Trigger - 发送消息 # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: kafka-trigger-sensor spec: dependencies: - name: kafka-dep eventSourceName: kafka-source eventName: my-topic triggers: - template: name: kafka-trigger kafka: url: kafka.default.svc.cluster.local:9092 topic: result-topic partition: 0 parameters: - src: dependencyName: kafka-dep dataKey: body.message dest: payload 📌 4. AWS Lambda Trigger - 执行函数 # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: lambda-trigger-sensor spec: dependencies: - name: webhook-dep eventSourceName: webhook-source eventName: webhook triggers: - template: name: call-lambda awsLambda: functionName: myLambdaFunction region: us-west-2 accessKey: key: accesskey name: aws-creds secretKey: key: secretkey name: aws-creds payload: - src: dependencyName: webhook-dep dataKey: body dest: body.message 📌 5. Argo Workflow Trigger（简写方式） # apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: simple-workflow-sensor spec: dependencies: - name: git-push eventSourceName: github-source eventName: push triggers: - template: name: simple-workflow argoWorkflow: group: argoproj.io version: v1alpha1 resource: workflows operation: create source: inline: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: gitops- spec: entrypoint: hello templates: - name: hello container: image: alpine command: [\u0026#34;echo\u0026#34;] args: [\u0026#34;Git pushed, deploying...\u0026#34;] 📚 总结 # 每种 Trigger 类型都可配合任意 EventSource，如 Webhook/Kafka/S3/Cron 等使用。EventBus 提供传输通道（默认 NATS），Sensor 管理依赖和事件逻辑，Trigger 执行实际动作。\n更多模板参考：https://github.com/argoproj/argo-events/tree/master/examples\n"},{"id":99,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/argo/argo-rollout/example/","title":"Example","section":"argo rollout","content":" 🚀 Argo CD 生产环境最复杂示例（多服务、多集群、多环境、Helm + Rollouts） # 本示例展示如何使用 Argo CD 管理多个服务、环境和集群，结合 Helm、Rollouts、Project、RBAC、ApplicationSet 构建完整 GitOps 流水线。\n📁 Git 仓库结构约定 # apps/ ├── svc-a/ │ └── charts/ │ ├── dev/values.yaml │ ├── stage/values.yaml │ └── prod/values.yaml ├── svc-b/ │ └── charts/ │ ├── dev/values.yaml │ └── prod/values.yaml └── rollout-templates/ └── nginx-rollout.yaml 🧩 1. AppProject 定义（多服务隔离） # apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: microservices namespace: argocd spec: description: Production microservice apps sourceRepos: - https://github.com/your-org/gitops-repo.git destinations: - namespace: svc-* server: https://kubernetes.default.svc - namespace: svc-* server: https://k8s-prod.example.com clusterResourceWhitelist: - group: \u0026#34;*\u0026#34; kind: \u0026#34;*\u0026#34; namespaceResourceWhitelist: - group: \u0026#34;*\u0026#34; kind: \u0026#34;*\u0026#34; ⚙️ 2. ApplicationSet（服务 x 环境 x 集群 x Rollout） # apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: multi-svc-prod namespace: argocd spec: generators: - matrix: generators: - git: repoURL: https://github.com/your-org/gitops-repo.git revision: main directories: - path: apps/*/charts/* - list: elements: - env: dev cluster: https://kubernetes.default.svc - env: prod cluster: https://k8s-prod.example.com template: metadata: name: \u0026#39;{{path[1]}}-{{env}}\u0026#39; spec: project: microservices source: repoURL: https://github.com/your-org/gitops-repo.git targetRevision: main path: apps/{{path[1]}}/charts/{{env}} helm: valueFiles: - values.yaml destination: server: \u0026#39;{{cluster}}\u0026#39; namespace: svc-{{path[1]}}-{{env}} syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true - ApplyOutOfSyncOnly=true 🔁 3. 示例服务使用 Rollouts（Helm Chart 中包含 rollout.yaml） # 在 values.yaml 中控制是否启用 Rollout 类型资源替代 Deployment。\nrollout: enabled: true strategy: canary canary: steps: - setWeight: 10 - pause: {duration: 1m} - setWeight: 100 analysis: enabled: true templateName: success-rate-check Chart 模板结构参考 nginx-rollout.yaml：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: {{ .Release.Name }} spec: strategy: canary: steps: - setWeight: 10 - pause: {duration: 1m} 🔐 4. RBAC 限制 # apiVersion: v1 kind: ConfigMap metadata: name: argocd-rbac-cm namespace: argocd data: policy.csv: | g, devops-team, role:microservices-admin p, role:microservices-admin, applications, *, microservices/*, allow 🔍 5. 集成建议 # Prometheus + Grafana：用于监控 Rollout 状态与指标分析 Istio：用于金丝雀流量控制 Dex + OIDC：用户认证 + 单点登录 Argo Notifications：通知部署状态变化 Loki/EFK：日志审计 Cert-Manager + Ingress：HTTPS 访问 Argo CD 界面 🧠 总结功能覆盖 # 模块 是否包含 多服务支持 ✅ 多环境（dev/prod） ✅ 多集群部署 ✅ Helm 管理配置 ✅ Canary Rollout + Prometheus ✅ 自动同步、自愈、命名空间自动创建 ✅ RBAC + Project + SSO ✅ 可观测性与通知 ✅ ApplicationSet 动态生成 ✅ 📚 参考文档 # Argo CD ApplicationSet：https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/ Argo Rollouts：https://argo-rollouts.readthedocs.io/ Prometheus Metrics + AnalysisRun：https://argo-rollouts.readthedocs.io/en/stable/features/analysis/ "},{"id":100,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/harbor/example/","title":"Example","section":"Harbor","content":" 🧪 Harbor 最复杂的生产使用示例 # 本示例演示在企业级生产环境中使用 Harbor 的复杂集成与最佳实践。该案例包括多数据中心部署、Keycloak 单点登录、CI/CD、镜像签名、漏洞扫描、多租户、异地灾备等完整场景。\n🏭 场景背景 # 企业规模：跨国公司 数据中心：北京、上海、新加坡 用户管理：Keycloak + AD 同步 镜像类型：容器镜像、Helm Chart、AI 模型 DevOps 工具：GitLab CI、Argo CD、Cosign 安全要求：镜像签名、漏洞扫描、RBAC、审计日志 🧰 组件部署架构 # +---------------------------+ +---------------------------+ | Developer (GitLab) | | SSO (Keycloak + AD) | +-------------+-------------+ +-------------+-------------+ | | v | GitLab CI/CD Runner | | | v v +--------+---------+ +-----------+-----------+ | Harbor (主) | \u0026lt;------------\u0026gt; | Keycloak Realm | | harbor.beijing | | realm: harbor | +--------+---------+ +------------------------+ | 镜像复制策略 (双向) v +--------+---------+ | Harbor (备) | | harbor.singapore | +------------------+ 🔐 安全配置 # 1. OIDC 登录与权限控制 # 启用 Keycloak OIDC 模式 Harbor 设置为 auth_mode: oidc Keycloak 同步 AD 用户，分组映射 Harbor 项目 Harbor 内部使用 RBAC 精细化授权（例如 project-admin, developer, guest） 2. 镜像签名（Cosign） # CI/CD 阶段使用 Cosign 对构建好的镜像签名： cosign sign --key k8s://default/mykey docker.io/acme/app:1.0.0 Harbor 启用镜像验证策略，确保部署前必须验证签名 3. 漏洞扫描（Trivy） # Harbor 启用 Trivy Scanner 配置上传即扫描策略 结合 Argo CD 实现漏洞阻断部署 4. 审计日志与存储加密 # 审计日志开启，导出至 ELK 系统 Harbor 存储后端为 S3，开启服务端加密（SSE） 🔄 镜像同步策略 # Harbor 主节点（北京）配置多个 replication rule： 同步至上海 Harbor（延迟 5s） 同步至新加坡 Harbor（用于灾备） 同步模式：Push-based，定时触发 支持双向同步（防止单点） 🚀 DevOps 流程集成 # CI 阶段（GitLab） # build: script: - docker build -t harbor.beijing.company.com/dev/app:$CI_COMMIT_SHA . - docker push harbor.beijing.company.com/dev/app:$CI_COMMIT_SHA - cosign sign --key k8s://default/signkey harbor.beijing.company.com/dev/app:$CI_COMMIT_SHA CD 阶段（Argo CD） # Argo CD 部署前校验镜像签名和扫描结果： 使用 Gatekeeper + OPA 策略 不合规镜像拒绝部署 📦 多类型镜像支持 # 容器镜像（标准） Helm Chart 仓库（开启 chartmuseum） AI 模型仓库（通过 OCI artifact 支持自定义类型） 📊 多租户管理 # 每个 BU（业务单元）对应一个 Harbor 项目 权限通过 Keycloak Group 自动同步 各项目启用资源配额（Quota）控制 🌐 灾备与高可用 # Harbor 主节点高可用部署（3 节点 Nginx + Keepalived + shared DB） 镜像异地同步 + 数据定期备份 Redis、数据库外部化，便于迁移和监控 ✅ 总结 # 此示例演示了如何在真实、复杂的企业生产环境中部署 Harbor，并与身份管理、CI/CD、安全机制、镜像治理等体系深度集成，构建一个稳定、安全、高效的镜像管理平台。\n"},{"id":101,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/harbor/install/","title":"Install","section":"Harbor","content":" 🚀 Harbor 生产环境安装与 Keycloak 对接指南 # 🏗️ 一、生产环境 Harbor 安装步骤 # 1. 系统准备 # 操作系统：Ubuntu 20.04 / CentOS 7+ 环境要求： CPU ≥ 2 核 内存 ≥ 4 GB 磁盘 ≥ 40 GB 安装依赖： sudo apt update \u0026amp;\u0026amp; sudo apt install docker.io docker-compose -y 2. 下载 Harbor 安装包 # wget https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-offline-installer-v2.10.0.tgz # 解压 tar -zxvf harbor-offline-installer-*.tgz cd harbor 3. 配置 harbor.yml # 编辑 harbor.yml 关键配置项：\nhostname: harbor.yourdomain.com https: port: 443 certificate: /path/to/cert.pem private_key: /path/to/key.pem harbor_admin_password: YourStrongAdminPass auth_mode: oidc oidc_provider_name: keycloak oidc_endpoint: https://keycloak.yourdomain.com/realms/harbor/.well-known/openid-configuration oidc_client_id: harbor-client oidc_client_secret: \u0026lt;your-client-secret\u0026gt; oidc_scope: openid,profile,email,groups oidc_verify_cert: true oidc_auto_onboard: true 4. 安装 Harbor # ./install.sh 🧩 二、配置 Keycloak 与 Harbor 对接 # 1. 创建 Realm # 名称建议为 harbor 2. 创建 Client # Client ID: harbor-client Access Type: confidential Root URL: https://harbor.yourdomain.com Redirect URI: https://harbor.yourdomain.com/c/oidc/callback 3. 启用功能 # 启用以下选项： Standard Flow Enabled Direct Access Grants Enabled Service Accounts Enabled 4. 映射用户组（可选） # 添加 Mapper： Name: groups Token Claim Name: groups Claim JSON Type: String Add to ID token / access token: ✅ 🔐 三、安全建议 # 项目 建议 TLS 证书 使用 CA 签发证书 镜像扫描 启用 Trivy 漏洞扫描 数据库外置 使用外部 PostgreSQL 提升可维护性 镜像签名 配合 Cosign / Notary 实现镜像签名 RBAC 权限控制 Keycloak 分组 + Harbor 项目权限 日志与审计 启用 audit.log 并定期备份 ✅ 四、验证流程 # 启动 Harbor 并访问 https://harbor.yourdomain.com 点击登录，将跳转到 Keycloak 页面 登录后自动完成注册并进入 Harbor 控制台 📦 五、补充建议 # Harbor 支持高可用部署，可配合 Nginx Ingress + Keepalived 实现 Harbor 支持 Helm Charts 管理，可结合 GitOps 工具使用 建议定期更新 Trivy 数据库，保持漏洞扫描能力的时效性 "},{"id":102,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/harbor/introduce/","title":"Introduce","section":"Harbor","content":" 🚢 Harbor 介绍文档 # 📌 一、Harbor 简介 # Harbor 是一个用于存储和分发容器镜像的企业级 Registry 服务，由 VMware 开源。它在 Docker Registry 的基础上扩展了角色访问控制、镜像复制、审计日志、镜像扫描、安全策略等企业功能，支持 Helm Chart 的托管，适用于多租户场景。\n⚙️ 二、Harbor 的原理 # Harbor 是一个基于 RESTful API 的架构，依赖于底层的 Docker Registry，提供增强的访问控制、安全机制、镜像扫描与复制等能力。它将镜像存储、权限认证、用户管理、审计日志、Web UI 和第三方集成模块有效结合，构建了一个全面的镜像管理平台。\n核心原理包括：\n🧑‍💻 用户通过 Web UI 或 CLI 登录 Harbor，获取 token 🔐 使用 token 进行权限控制和身份认证 🔁 支持多种镜像复制策略，实现跨数据中心的镜像同步 🛡️ 对接 Trivy 等扫描器实现漏洞检测 🧱 三、Harbor 架构图 # +---------------------+ | Web UI | +----------+----------+ | +----------v----------+ | API Server |\u0026lt;------------+ +----------+----------+ | | | +----------v----------+ +---------v----------+ | Core Services | | Authentication | | (Project/User Mgmt) | | (LDAP/DB/OIDC) | +----------+----------+ +---------+----------+ | | +----------v----------+ +---------v----------+ | Job Service | | Scanner (Trivy) | +----------+----------+ +--------------------+ | +----------v----------+ | Registry (Docker) | +----------+----------+ | +----------v----------+ | Storage Backend | +---------------------+ 🧰 四、Harbor 的作用 # 🏢 私有镜像仓库：为企业提供安全的镜像仓库，防止公共仓库的不稳定和安全风险。 🧑‍🔧 访问控制和权限管理：支持基于角色的访问控制 (RBAC)，确保资源隔离和权限安全。 🌐 镜像复制与多节点部署：支持跨 Harbor 实例的镜像同步，便于多数据中心部署与备份。 🕵️‍♂️ 镜像漏洞扫描：集成 Trivy 等扫描工具，提升镜像安全性。 📝 审计日志：记录所有用户操作，方便安全审计与问题追踪。 📦 支持 Helm Chart：不仅支持容器镜像，还支持 Helm Chart 仓库管理。 🧾 五、总结 # Harbor 作为企业级容器镜像管理平台，是构建安全、可靠、高效 DevOps 流水线的关键组件。通过其丰富的功能和良好的扩展性，为容器化生产环境提供了重要支持。\n"},{"id":103,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/helm-install-postgresql/","title":"Helm Install Postgresql","section":"云原生","content":" 使用 Helm 安装 PostgreSQL 到 Kubernetes # 前提条件 # 已安装并配置好的 Kubernetes 集群 本地已安装并配置好 kubectl 安装了 Helm v3+ 步骤一：添加 Bitnami Helm 仓库 # helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update 步骤二：创建命名空间（可选） # kubectl create namespace postgres 步骤三：安装 PostgreSQL # helm install my-postgresql \\ --namespace postgres \\ --set auth.postgresPassword=your-password \\ bitnami/postgresql 你可以根据需要修改参数，如：\nauth.postgresPassword：设置 PostgreSQL 的密码 primary.persistence.enabled=true/false：是否启用数据持久化 primary.resources.requests：设置 CPU/内存请求资源 步骤四：验证安装 # kubectl get pods -n postgres kubectl get svc -n postgres 步骤五：连接到 PostgreSQL # 方式一：通过 Pod 内部连接 # kubectl run -i --tty --rm psql-client --image=bitnami/postgresql --namespace postgres \\ --env=\u0026#34;PGPASSWORD=your-password\u0026#34; \\ --command -- psql -h my-postgresql.postgres.svc.cluster.local -U postgres 方式二：通过端口转发（本地连接） # kubectl port-forward --namespace postgres svc/my-postgresql 5432:5432 然后使用本地工具连接：\npsql -h 127.0.0.1 -U postgres -p 5432 密码为 your-password\n步骤六：卸载 PostgreSQL（可选） # helm uninstall my-postgresql -n postgres kubectl delete namespace postgres 附录：常用 Helm 参数 # 参数 描述 示例 auth.postgresPassword 设置 PostgreSQL 密码 your-password primary.persistence.enabled 启用持久化 true primary.persistence.size 存储大小 8Gi primary.resources.requests.memory 内存请求 512Mi primary.resources.requests.cpu CPU 请求 250m 更多参数详见： Bitnami PostgreSQL Helm Chart 文档\n"},{"id":104,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/keycloak/install/","title":"Install","section":"Keycloak","content":" 🏗️ Keycloak 生产环境部署指南 # 📦 部署方式概览 # Keycloak 支持多种部署方式，常见包括：\nDocker / Podman 容器部署 Kubernetes / OpenShift 部署（推荐生产使用） 使用独立 JAR 文件运行 Helm Chart 部署（适合 Kubernetes 环境） 本指南以 Kubernetes 环境 + PostgreSQL 数据库为例进行说明。\n🔧 环境准备 # 系统要求 # Kubernetes 集群（v1.21+） Ingress Controller（如 nginx-ingress） PostgreSQL 数据库 持久化存储（PVC） Cert Manager（可选，用于 TLS） 建议资源配置 # 组件 CPU 内存 Keycloak 2核 4Gi PostgreSQL 1核 2Gi 🐳 使用 Helm 部署 Keycloak # 1️⃣ 添加 Helm 仓库 # helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update 2️⃣ 安装 PostgreSQL（可选，或使用外部 DB） # helm install keycloak-postgresql bitnami/postgresql \\ --set auth.postgresPassword=yourpassword \\ --set primary.persistence.enabled=true 3️⃣ 安装 Keycloak # helm install keycloak bitnami/keycloak \\ --set auth.adminUser=admin \\ --set auth.adminPassword=adminpassword \\ --set externalDatabase.host=keycloak-postgresql \\ --set externalDatabase.user=postgres \\ --set externalDatabase.password=yourpassword \\ --set proxy=edge \\ --set ingress.enabled=true \\ --set ingress.hostname=keycloak.example.com 🔒 TLS 配置（可选） # 如果集群中部署了 cert-manager，可通过如下方式配置 TLS：\ningress: enabled: true hostname: keycloak.example.com tls: true annotations: cert-manager.io/cluster-issuer: letsencrypt-prod extraTls: - hosts: - keycloak.example.com secretName: keycloak-tls 📁 数据持久化配置 # Keycloak 默认开启持久化，确保 statefulset 挂载了 PVC：\npersistence: enabled: true size: 8Gi storageClass: standard 🩺 健康检查与监控 # 默认开启 liveness/readiness probe 可通过 Prometheus Operator 集成 Keycloak Exporter 实现监控 🛠️ 常见问题排查 # 问题 解决方法 Admin 页面打不开 检查 ingress 或 admin 密码是否正确 数据丢失 确保 PVC 正常挂载并配置持久化 登录失败 检查数据库连接配置是否正确 📚 参考链接 # 官方文档： https://www.keycloak.org/documentation Helm Chart： https://artifacthub.io/packages/helm/bitnami/keycloak GitHub： https://github.com/keycloak/keycloak ✅ 建议将 Keycloak 与 LDAP/AD 集成，并定期备份数据库，启用高可用部署以增强稳定性。\n"},{"id":105,"href":"/docs/%E4%BA%91%E5%8E%9F%E7%94%9F/keycloak/introduce/","title":"Introduce","section":"Keycloak","content":" 🛡️ Keycloak 简介与架构 # 📌 什么是 Keycloak？ # Keycloak 是一个开源的身份和访问管理 (IAM) 解决方案，支持单点登录 (SSO)、身份认证、授权和用户管理功能，适用于现代应用程序与服务。\n它由 Red Hat 领导维护，广泛用于微服务架构、安全网关、企业级权限控制等场景。\n🧠 Keycloak 的核心原理 # Keycloak 基于 OAuth 2.0、OpenID Connect 和 SAML 2.0 等标准协议构建，允许开发者将认证交给 Keycloak 处理，从而简化应用程序的身份安全实现。\n🔐 身份认证：用户通过 Keycloak 登录，获取访问令牌。 🆔 授权管理：定义角色（Role）与权限（Permission）来控制资源访问。 🔁 单点登录与登出：一次登录，访问多个系统；统一登出清除所有登录状态。 📦 多协议支持：支持标准 OAuth2、OIDC、SAML 协议，兼容性强。 🏗️ Keycloak 架构图 # graph LR A[用户] --\u0026gt;|浏览器访问| B(Keycloak 登录界面) B --\u0026gt; C[身份验证（用户名/密码、社交登录等）] C --\u0026gt; D[Keycloak Server] D --\u0026gt;|Token| E[客户端应用] E --\u0026gt;|访问| F[受保护资源] 或简化为：\n[用户] --浏览器--\u0026gt; [Keycloak Server] --Token--\u0026gt; [客户端] --请求--\u0026gt; [受保护服务] ⚙️ Keycloak 的组件 # 🏛️ Realm：一个逻辑隔离的空间，定义了一组用户、角色和客户端。 👥 User：系统中的用户，可以直接创建或集成 LDAP、AD。 🎭 Client：接入 Keycloak 的应用，支持配置访问类型与协议。 🧩 Role / Group：用于权限控制和组织管理。 🔌 Identity Provider (IdP)：第三方身份源（如 Google、GitHub 等）。 🎯 Keycloak 的作用 # ✅ 实现统一认证与授权：简化多系统的登录逻辑。 🔄 提供单点登录能力：一次登录，处处通行。 🔐 支持精细化权限控制：通过角色与策略进行访问控制。 🧬 集成多种身份源：支持 LDAP、Active Directory、社交账号。 📊 提供可视化管理界面：方便配置用户、角色、客户端。 📤 支持 REST API 与 Admin CLI：可集成自动化平台。 📚 总结 # Keycloak 是一款功能强大的 IAM 工具，适用于从小型项目到企业级系统的认证与授权需求。通过标准协议、灵活配置以及丰富的集成能力，为现代应用安全保驾护航。\n🔗 相关链接：\n官网： https://www.keycloak.org GitHub： https://github.com/keycloak/keycloak 文档： https://www.keycloak.org/documentation.html "}]